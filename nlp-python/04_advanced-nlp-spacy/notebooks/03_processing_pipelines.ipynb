{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "\n",
    "DATA_PATH = \"../data/raw/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7fc7461a9580>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7fc7461a9a60>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7fc7469522e0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7fc747b23f80>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7fc745867700>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7fc746952190>)]\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "print(nlp.pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom component in pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 5 tokens long.\n"
     ]
    }
   ],
   "source": [
    "# Define the custom component\n",
    "@Language.component(\"length_component\")\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English model and Add the component first in the pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span, Token, Doc\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 8 tokens long.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component(doc):\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    # and overwrite the doc.ents with the matched spans\n",
    "    doc.ents = [\n",
    "        Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matcher(doc)\n",
    "    ]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the 'ner' component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 5 tokens long.\n",
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "# Register the Token extension attribute 'is_country' with the default value False\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 9 tokens long.\n",
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 9 tokens long.\n",
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "\n",
    "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
    "Doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute\n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 8 tokens long.\n",
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return \"<{tag}>{text}</{tag}>\".format(tag=tag, text=span.text)\n",
    "\n",
    "\n",
    "# Register the Span property extension 'to_html' with the method to_html\n",
    "Span.set_extension(\"to_html\", method=to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name 'strong'\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entities & extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = {\n",
    "    \"Afghanistan\": \"Kabul\",\n",
    "    \"Albania\": \"Tirana\",\n",
    "    \"Algeria\": \"Algiers\",\n",
    "    \"American Samoa\": \"Pago Pago\",\n",
    "    \"Andorra\": \"Andorra la Vella\",\n",
    "    \"Angola\": \"Luanda\",\n",
    "    \"Anguilla\": \"The Valley\",\n",
    "    \"Antarctica\": \"\",\n",
    "    \"Antigua and Barbuda\": \"Saint John's\",\n",
    "    \"Argentina\": \"Buenos Aires\",\n",
    "    \"Armenia\": \"Yerevan\",\n",
    "    \"Aruba\": \"Oranjestad\",\n",
    "    \"Australia\": \"Canberra\",\n",
    "    \"Austria\": \"Vienna\",\n",
    "    \"Azerbaijan\": \"Baku\",\n",
    "    \"Bahamas\": \"Nassau\",\n",
    "    \"Bahrain\": \"Manama\",\n",
    "    \"Bangladesh\": \"Dhaka\",\n",
    "    \"Barbados\": \"Bridgetown\",\n",
    "    \"Belarus\": \"Minsk\",\n",
    "    \"Belgium\": \"Brussels\",\n",
    "    \"Belize\": \"Belmopan\",\n",
    "    \"Benin\": \"Porto-Novo\",\n",
    "    \"Bermuda\": \"Hamilton\",\n",
    "    \"Bhutan\": \"Thimphu\",\n",
    "    \"Bolivia (Plurinational State of)\": \"Sucre\",\n",
    "    \"Bonaire, Sint Eustatius and Saba\": \"Kralendijk\",\n",
    "    \"Bosnia and Herzegovina\": \"Sarajevo\",\n",
    "    \"Botswana\": \"Gaborone\",\n",
    "    \"Bouvet Island\": \"\",\n",
    "    \"Brazil\": \"Brasília\",\n",
    "    \"British Indian Ocean Territory\": \"Diego Garcia\",\n",
    "    \"Brunei Darussalam\": \"Bandar Seri Begawan\",\n",
    "    \"Bulgaria\": \"Sofia\",\n",
    "    \"Burkina Faso\": \"Ouagadougou\",\n",
    "    \"Burundi\": \"Bujumbura\",\n",
    "    \"Cabo Verde\": \"Praia\",\n",
    "    \"Cambodia\": \"Phnom Penh\",\n",
    "    \"Cameroon\": \"Yaoundé\",\n",
    "    \"Canada\": \"Ottawa\",\n",
    "    \"Cayman Islands\": \"George Town\",\n",
    "    \"Central African Republic\": \"Bangui\",\n",
    "    \"Chad\": \"N'Djamena\",\n",
    "    \"Chile\": \"Santiago\",\n",
    "    \"China\": \"Beijing\",\n",
    "    \"Christmas Island\": \"Flying Fish Cove\",\n",
    "    \"Cocos (Keeling) Islands\": \"West Island\",\n",
    "    \"Colombia\": \"Bogotá\",\n",
    "    \"Comoros\": \"Moroni\",\n",
    "    \"Congo\": \"Brazzaville\",\n",
    "    \"Congo (Democratic Republic of the)\": \"Kinshasa\",\n",
    "    \"Cook Islands\": \"Avarua\",\n",
    "    \"Costa Rica\": \"San José\",\n",
    "    \"Croatia\": \"Zagreb\",\n",
    "    \"Cuba\": \"Havana\",\n",
    "    \"Curaçao\": \"Willemstad\",\n",
    "    \"Cyprus\": \"Nicosia\",\n",
    "    \"Czech Republic\": \"Prague\",\n",
    "    \"Côte d'Ivoire\": \"Yamoussoukro\",\n",
    "    \"Denmark\": \"Copenhagen\",\n",
    "    \"Djibouti\": \"Djibouti\",\n",
    "    \"Dominica\": \"Roseau\",\n",
    "    \"Dominican Republic\": \"Santo Domingo\",\n",
    "    \"Ecuador\": \"Quito\",\n",
    "    \"Egypt\": \"Cairo\",\n",
    "    \"El Salvador\": \"San Salvador\",\n",
    "    \"Equatorial Guinea\": \"Malabo\",\n",
    "    \"Eritrea\": \"Asmara\",\n",
    "    \"Estonia\": \"Tallinn\",\n",
    "    \"Ethiopia\": \"Addis Ababa\",\n",
    "    \"Falkland Islands (Malvinas)\": \"Stanley\",\n",
    "    \"Faroe Islands\": \"Tórshavn\",\n",
    "    \"Fiji\": \"Suva\",\n",
    "    \"Finland\": \"Helsinki\",\n",
    "    \"France\": \"Paris\",\n",
    "    \"French Guiana\": \"Cayenne\",\n",
    "    \"French Polynesia\": \"Papeetē\",\n",
    "    \"French Southern Territories\": \"Port-aux-Français\",\n",
    "    \"Gabon\": \"Libreville\",\n",
    "    \"Gambia\": \"Banjul\",\n",
    "    \"Georgia\": \"Tbilisi\",\n",
    "    \"Germany\": \"Berlin\",\n",
    "    \"Ghana\": \"Accra\",\n",
    "    \"Gibraltar\": \"Gibraltar\",\n",
    "    \"Greece\": \"Athens\",\n",
    "    \"Greenland\": \"Nuuk\",\n",
    "    \"Grenada\": \"St. George's\",\n",
    "    \"Guadeloupe\": \"Basse-Terre\",\n",
    "    \"Guam\": \"Hagåtña\",\n",
    "    \"Guatemala\": \"Guatemala City\",\n",
    "    \"Guernsey\": \"St. Peter Port\",\n",
    "    \"Guinea\": \"Conakry\",\n",
    "    \"Guinea-Bissau\": \"Bissau\",\n",
    "    \"Guyana\": \"Georgetown\",\n",
    "    \"Haiti\": \"Port-au-Prince\",\n",
    "    \"Heard Island and McDonald Islands\": \"\",\n",
    "    \"Holy See\": \"Rome\",\n",
    "    \"Honduras\": \"Tegucigalpa\",\n",
    "    \"Hong Kong\": \"City of Victoria\",\n",
    "    \"Hungary\": \"Budapest\",\n",
    "    \"Iceland\": \"Reykjavík\",\n",
    "    \"India\": \"New Delhi\",\n",
    "    \"Indonesia\": \"Jakarta\",\n",
    "    \"Iran (Islamic Republic of)\": \"Tehran\",\n",
    "    \"Iraq\": \"Baghdad\",\n",
    "    \"Ireland\": \"Dublin\",\n",
    "    \"Isle of Man\": \"Douglas\",\n",
    "    \"Israel\": \"Jerusalem\",\n",
    "    \"Italy\": \"Rome\",\n",
    "    \"Jamaica\": \"Kingston\",\n",
    "    \"Japan\": \"Tokyo\",\n",
    "    \"Jersey\": \"Saint Helier\",\n",
    "    \"Jordan\": \"Amman\",\n",
    "    \"Kazakhstan\": \"Astana\",\n",
    "    \"Kenya\": \"Nairobi\",\n",
    "    \"Kiribati\": \"South Tarawa\",\n",
    "    \"Korea (Democratic People's Republic of)\": \"Pyongyang\",\n",
    "    \"Korea (Republic of)\": \"Seoul\",\n",
    "    \"Kuwait\": \"Kuwait City\",\n",
    "    \"Kyrgyzstan\": \"Bishkek\",\n",
    "    \"Lao People's Democratic Republic\": \"Vientiane\",\n",
    "    \"Latvia\": \"Riga\",\n",
    "    \"Lebanon\": \"Beirut\",\n",
    "    \"Lesotho\": \"Maseru\",\n",
    "    \"Liberia\": \"Monrovia\",\n",
    "    \"Libya\": \"Tripoli\",\n",
    "    \"Liechtenstein\": \"Vaduz\",\n",
    "    \"Lithuania\": \"Vilnius\",\n",
    "    \"Luxembourg\": \"Luxembourg\",\n",
    "    \"Macao\": \"\",\n",
    "    \"Macedonia (the former Yugoslav Republic of)\": \"Skopje\",\n",
    "    \"Madagascar\": \"Antananarivo\",\n",
    "    \"Malawi\": \"Lilongwe\",\n",
    "    \"Malaysia\": \"Kuala Lumpur\",\n",
    "    \"Maldives\": \"Malé\",\n",
    "    \"Mali\": \"Bamako\",\n",
    "    \"Malta\": \"Valletta\",\n",
    "    \"Marshall Islands\": \"Majuro\",\n",
    "    \"Martinique\": \"Fort-de-France\",\n",
    "    \"Mauritania\": \"Nouakchott\",\n",
    "    \"Mauritius\": \"Port Louis\",\n",
    "    \"Mayotte\": \"Mamoudzou\",\n",
    "    \"Mexico\": \"Mexico City\",\n",
    "    \"Micronesia (Federated States of)\": \"Palikir\",\n",
    "    \"Moldova (Republic of)\": \"Chișinău\",\n",
    "    \"Monaco\": \"Monaco\",\n",
    "    \"Mongolia\": \"Ulan Bator\",\n",
    "    \"Montenegro\": \"Podgorica\",\n",
    "    \"Montserrat\": \"Plymouth\",\n",
    "    \"Morocco\": \"Rabat\",\n",
    "    \"Mozambique\": \"Maputo\",\n",
    "    \"Myanmar\": \"Naypyidaw\",\n",
    "    \"Namibia\": \"Windhoek\",\n",
    "    \"Nauru\": \"Yaren\",\n",
    "    \"Nepal\": \"Kathmandu\",\n",
    "    \"Netherlands\": \"Amsterdam\",\n",
    "    \"New Caledonia\": \"Nouméa\",\n",
    "    \"New Zealand\": \"Wellington\",\n",
    "    \"Nicaragua\": \"Managua\",\n",
    "    \"Niger\": \"Niamey\",\n",
    "    \"Nigeria\": \"Abuja\",\n",
    "    \"Niue\": \"Alofi\",\n",
    "    \"Norfolk Island\": \"Kingston\",\n",
    "    \"Northern Mariana Islands\": \"Saipan\",\n",
    "    \"Norway\": \"Oslo\",\n",
    "    \"Oman\": \"Muscat\",\n",
    "    \"Pakistan\": \"Islamabad\",\n",
    "    \"Palau\": \"Ngerulmud\",\n",
    "    \"Palestine, State of\": \"Ramallah\",\n",
    "    \"Panama\": \"Panama City\",\n",
    "    \"Papua New Guinea\": \"Port Moresby\",\n",
    "    \"Paraguay\": \"Asunción\",\n",
    "    \"Peru\": \"Lima\",\n",
    "    \"Philippines\": \"Manila\",\n",
    "    \"Pitcairn\": \"Adamstown\",\n",
    "    \"Poland\": \"Warsaw\",\n",
    "    \"Portugal\": \"Lisbon\",\n",
    "    \"Puerto Rico\": \"San Juan\",\n",
    "    \"Qatar\": \"Doha\",\n",
    "    \"Republic of Kosovo\": \"Pristina\",\n",
    "    \"Romania\": \"Bucharest\",\n",
    "    \"Russian Federation\": \"Moscow\",\n",
    "    \"Rwanda\": \"Kigali\",\n",
    "    \"Réunion\": \"Saint-Denis\",\n",
    "    \"Saint Barthélemy\": \"Gustavia\",\n",
    "    \"Saint Helena, Ascension and Tristan da Cunha\": \"Jamestown\",\n",
    "    \"Saint Kitts and Nevis\": \"Basseterre\",\n",
    "    \"Saint Lucia\": \"Castries\",\n",
    "    \"Saint Martin (French part)\": \"Marigot\",\n",
    "    \"Saint Pierre and Miquelon\": \"Saint-Pierre\",\n",
    "    \"Saint Vincent and the Grenadines\": \"Kingstown\",\n",
    "    \"Samoa\": \"Apia\",\n",
    "    \"San Marino\": \"City of San Marino\",\n",
    "    \"Sao Tome and Principe\": \"São Tomé\",\n",
    "    \"Saudi Arabia\": \"Riyadh\",\n",
    "    \"Senegal\": \"Dakar\",\n",
    "    \"Serbia\": \"Belgrade\",\n",
    "    \"Seychelles\": \"Victoria\",\n",
    "    \"Sierra Leone\": \"Freetown\",\n",
    "    \"Singapore\": \"Singapore\",\n",
    "    \"Sint Maarten (Dutch part)\": \"Philipsburg\",\n",
    "    \"Slovakia\": \"Bratislava\",\n",
    "    \"Slovenia\": \"Ljubljana\",\n",
    "    \"Solomon Islands\": \"Honiara\",\n",
    "    \"Somalia\": \"Mogadishu\",\n",
    "    \"South Africa\": \"Pretoria\",\n",
    "    \"South Georgia and the South Sandwich Islands\": \"King Edward Point\",\n",
    "    \"South Sudan\": \"Juba\",\n",
    "    \"Spain\": \"Madrid\",\n",
    "    \"Sri Lanka\": \"Colombo\",\n",
    "    \"Sudan\": \"Khartoum\",\n",
    "    \"Suriname\": \"Paramaribo\",\n",
    "    \"Svalbard and Jan Mayen\": \"Longyearbyen\",\n",
    "    \"Swaziland\": \"Lobamba\",\n",
    "    \"Sweden\": \"Stockholm\",\n",
    "    \"Switzerland\": \"Bern\",\n",
    "    \"Syrian Arab Republic\": \"Damascus\",\n",
    "    \"Taiwan\": \"Taipei\",\n",
    "    \"Tajikistan\": \"Dushanbe\",\n",
    "    \"Tanzania, United Republic of\": \"Dodoma\",\n",
    "    \"Thailand\": \"Bangkok\",\n",
    "    \"Timor-Leste\": \"Dili\",\n",
    "    \"Togo\": \"Lomé\",\n",
    "    \"Tokelau\": \"Fakaofo\",\n",
    "    \"Tonga\": \"Nuku'alofa\",\n",
    "    \"Trinidad and Tobago\": \"Port of Spain\",\n",
    "    \"Tunisia\": \"Tunis\",\n",
    "    \"Turkey\": \"Ankara\",\n",
    "    \"Turkmenistan\": \"Ashgabat\",\n",
    "    \"Turks and Caicos Islands\": \"Cockburn Town\",\n",
    "    \"Tuvalu\": \"Funafuti\",\n",
    "    \"Uganda\": \"Kampala\",\n",
    "    \"Ukraine\": \"Kiev\",\n",
    "    \"United Arab Emirates\": \"Abu Dhabi\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\": \"London\",\n",
    "    \"United States Minor Outlying Islands\": \"\",\n",
    "    \"United States of America\": \"Washington, D.C.\",\n",
    "    \"Uruguay\": \"Montevideo\",\n",
    "    \"Uzbekistan\": \"Tashkent\",\n",
    "    \"Vanuatu\": \"Port Vila\",\n",
    "    \"Venezuela (Bolivarian Republic of)\": \"Caracas\",\n",
    "    \"Viet Nam\": \"Hanoi\",\n",
    "    \"Virgin Islands (British)\": \"Road Town\",\n",
    "    \"Virgin Islands (U.S.)\": \"Charlotte Amalie\",\n",
    "    \"Wallis and Futuna\": \"Mata-Utu\",\n",
    "    \"Western Sahara\": \"El Aaiún\",\n",
    "    \"Yemen\": \"Sana'a\",\n",
    "    \"Zambia\": \"Lusaka\",\n",
    "    \"Zimbabwe\": \"Harare\",\n",
    "    \"Åland Islands\": \"Mariehamn\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "@Language.component(\"countries_component\")\n",
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label 'GPE' for all matches\n",
    "    doc.ents = [\n",
    "        Span(doc, start, end, label=\"GPE\") for match_id, start, end in matcher(doc)\n",
    "    ]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: capitals.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute 'capital' with the getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = [\n",
    "    \"McDonalds is my favorite restaurant.\",\n",
    "    \"Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..\",\n",
    "    \"People really still eat McDonalds :(\",\n",
    "    \"The McDonalds in Spain has chicken wings. My heart is so happy \",\n",
    "    \"@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P\",\n",
    "    \"please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D\",\n",
    "    \"This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "[]\n",
      "['terrible']\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the adjectives\n",
    "# for text in TEXTS:\n",
    "#     doc = nlp(text)\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([token.text for token in doc if token.pos_ == \"ADJ\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() () () () () () ()\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the entities\n",
    "# docs = [nlp(text) for text in TEXTS]\n",
    "docs = list(nlp.pipe(TEXTS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = [\"David Bowie\", \"Angela Merkel\", \"Lady Gaga\"]\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "# patterns = [nlp(person) for person in people]\n",
    "patterns = list(nlp.pipe(people))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = [\n",
    "    (\n",
    "        \"One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.\",\n",
    "        {\"author\": \"Franz Kafka\", \"book\": \"Metamorphosis\"},\n",
    "    ),\n",
    "    (\n",
    "        \"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n",
    "        {\"author\": \"Herman Melville\", \"book\": \"Moby-Dick or, The Whale\"},\n",
    "    ),\n",
    "    (\n",
    "        \"It was the best of times, it was the worst of times.\",\n",
    "        {\"author\": \"Charles Dickens\", \"book\": \"A Tale of Two Cities\"},\n",
    "    ),\n",
    "    (\n",
    "        \"The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.\",\n",
    "        {\"author\": \"Jack Kerouac\", \"book\": \"On the Road\"},\n",
    "    ),\n",
    "    (\n",
    "        \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
    "        {\"author\": \"George Orwell\", \"book\": \"1984\"},\n",
    "    ),\n",
    "    (\n",
    "        \"Nowadays people know the price of everything and the value of nothing.\",\n",
    "        {\"author\": \"Oscar Wilde\", \"book\": \"The Picture Of Dorian Gray\"},\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " — 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " — 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " — 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " — 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " — '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class and register the extensions 'author' and 'book'\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"book\", default=None)\n",
    "Doc.set_extension(\"author\", default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context[\"book\"]\n",
    "    doc._.author = context[\"author\"]\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, \"\\n\", \"— '{}' by {}\".format(doc._.book, doc._.author), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prime/.local/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "\n",
    "print([token.text for token in doc])\n",
    "\n",
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes(\"tagger\", \"parser\"):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53c3e8f2eb9682b6c71ce85616632b4da73b82d13cfbbe1554e54cc934f411d0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('dsenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
