{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Unsupervised Learning in Python\n","\n","## Clustering for dataset exploration\n","\n","### Clustering 2D points\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","DATA_PATH = \"../data/raw\"\n","\n","points = pd.read_csv(os.path.join(DATA_PATH, \"points.csv\")).values\n","new_points = pd.read_csv(os.path.join(DATA_PATH, \"new_points.csv\")).values\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import KMeans\n","from sklearn.cluster import KMeans\n","\n","# Create a KMeans instance with 3 clusters: model\n","model = KMeans(n_clusters=3)\n","\n","# Fit model to points\n","model.fit(points)\n","\n","# Determine the cluster labels of new_points: labels\n","labels = model.predict(new_points)\n","\n","# Print cluster labels of new_points\n","print(labels)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Inspect your clustering\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import pyplot\n","import matplotlib.pyplot as plt\n","\n","# Assign the columns of new_points: xs and ys\n","xs = new_points[:, 0]\n","ys = new_points[:, 1]\n","\n","# Make a scatter plot of xs and ys, using labels to define the colors\n","plt.scatter(xs, ys, c=labels, alpha=0.5)\n","\n","# Assign the cluster centers: centroids\n","centroids = model.cluster_centers_\n","\n","# Assign the columns of centroids: centroids_x, centroids_y\n","centroids_x = centroids[:, 0]\n","centroids_y = centroids[:, 1]\n","\n","# Make a scatter plot of centroids_x and centroids_y\n","plt.scatter(centroids_x, centroids_y, marker=\"D\", s=50)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### How many clusters of grain?\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["samples = pd.read_csv(os.path.join(DATA_PATH, \"samples.csv\")).values\n","ks = range(1, 6)\n","inertias = []\n","\n","for k in ks:\n","    # Create a KMeans instance with k clusters: model\n","    model = KMeans(n_clusters=k)\n","\n","    # Fit model to samples\n","    model.fit(samples)\n","\n","    # Append the inertia to the list of inertias\n","    inertias.append(model.inertia_)\n","\n","# Plot ks vs inertias\n","plt.plot(ks, inertias, \"-o\")\n","plt.xlabel(\"number of clusters, k\")\n","plt.ylabel(\"inertia\")\n","plt.xticks(ks)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluating the grain clustering\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["varieties = [\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a KMeans model with 3 clusters: model\n","model = KMeans(n_clusters=3)\n","\n","# Use fit_predict to fit model and obtain cluster labels: labels\n","labels = model.fit_predict(samples)\n","\n","# Create a DataFrame with labels and varieties as columns: df\n","df = pd.DataFrame({\"labels\": labels, \"varieties\": varieties})\n","\n","# Create crosstab: ct\n","ct = pd.crosstab(df[\"labels\"], df[\"varieties\"])\n","\n","# Display ct\n","print(ct)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Scaling fish data for clustering\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform the necessary imports\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","\n","# Create scaler: scaler\n","scaler = StandardScaler()\n","\n","# Create KMeans instance: kmeans\n","kmeans = KMeans(n_clusters=4)\n","\n","# Create pipeline: pipeline\n","pipeline = make_pipeline(scaler, kmeans)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Clustering the fish data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["samples = pd.read_csv(os.path.join(DATA_PATH, \"fish.csv\")).values\n","species = [\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Bream\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Roach\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Smelt\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","    \"Pike\",\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import pandas\n","import pandas as pd\n","\n","# Fit the pipeline to samples\n","pipeline.fit(samples)\n","\n","# Calculate the cluster labels: labels\n","labels = pipeline.predict(samples)\n","\n","# Create a DataFrame with labels and species as columns: df\n","df = pd.DataFrame({\"labels\":labels, \"species\": species})\n","\n","# Create crosstab: ct\n","ct = pd.crosstab(df[\"labels\"],df[\"species\"])\n","\n","# Display ct\n","print(ct)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Clustering stocks using KMeans\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["movements = pd.read_csv(os.path.join(DATA_PATH, \"movements.csv\")).values\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import Normalizer\n","from sklearn.preprocessing import Normalizer\n","\n","# Create a normalizer: normalizer\n","normalizer = Normalizer()\n","\n","# Create a KMeans model with 10 clusters: kmeans\n","kmeans = KMeans(n_clusters=10)\n","\n","# Make a pipeline chaining normalizer and kmeans: pipeline\n","pipeline = make_pipeline(normalizer, kmeans)\n","\n","# Fit pipeline to the daily price movements\n","pipeline.fit(movements)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Which stocks move together?\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["companies = [\n","    \"Apple\",\n","    \"AIG\",\n","    \"Amazon\",\n","    \"American express\",\n","    \"Boeing\",\n","    \"Bank of America\",\n","    \"British American Tobacco\",\n","    \"Canon\",\n","    \"Caterpillar\",\n","    \"Colgate-Palmolive\",\n","    \"ConocoPhillips\",\n","    \"Cisco\",\n","    \"Chevron\",\n","    \"DuPont de Nemours\",\n","    \"Dell\",\n","    \"Ford\",\n","    \"General Electrics\",\n","    \"Google/Alphabet\",\n","    \"Goldman Sachs\",\n","    \"GlaxoSmithKline\",\n","    \"Home Depot\",\n","    \"Honda\",\n","    \"HP\",\n","    \"IBM\",\n","    \"Intel\",\n","    \"Johnson & Johnson\",\n","    \"JPMorgan Chase\",\n","    \"Kimberly-Clark\",\n","    \"Coca Cola\",\n","    \"Lookheed Martin\",\n","    \"MasterCard\",\n","    \"McDonalds\",\n","    \"3M\",\n","    \"Microsoft\",\n","    \"Mitsubishi\",\n","    \"Navistar\",\n","    \"Northrop Grumman\",\n","    \"Novartis\",\n","    \"Pepsi\",\n","    \"Pfizer\",\n","    \"Procter Gamble\",\n","    \"Philip Morris\",\n","    \"Royal Dutch Shell\",\n","    \"SAP\",\n","    \"Schlumberger\",\n","    \"Sony\",\n","    \"Sanofi-Aventis\",\n","    \"Symantec\",\n","    \"Toyota\",\n","    \"Total\",\n","    \"Taiwan Semiconductor Manufacturing\",\n","    \"Texas instruments\",\n","    \"Unilever\",\n","    \"Valero Energy\",\n","    \"Walgreen\",\n","    \"Wells Fargo\",\n","    \"Wal-Mart\",\n","    \"Exxon\",\n","    \"Xerox\",\n","    \"Yahoo\",\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import pandas\n","import pandas as pd\n","\n","# Predict the cluster labels: labels\n","labels = pipeline.predict(movements)\n","\n","# Create a DataFrame aligning labels and companies: df\n","df = pd.DataFrame({\"labels\": labels, \"companies\": companies})\n","\n","# Display df sorted by cluster label\n","print(df.sort_values(by=\"labels\"))\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Visualization with hierarchical clustering and t-SNE\n","\n","### Hierarchical clustering of the grain data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["samples = pd.read_csv(os.path.join(DATA_PATH, \"samples.csv\")).values\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform the necessary imports\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","import matplotlib.pyplot as plt\n","\n","# Calculate the linkage: mergings\n","mergings = linkage(samples, method=\"complete\")\n","\n","# Plot the dendrogram, using varieties as labels\n","dendrogram(\n","    mergings,\n","    labels=varieties,\n","    leaf_rotation=90,\n","    leaf_font_size=6,\n",")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Hierarchies of stocks\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import normalize\n","from sklearn.preprocessing import normalize\n","\n","# Normalize the movements: normalized_movements\n","normalized_movements = normalize(movements)\n","\n","# Calculate the linkage: mergings\n","mergings = linkage(normalized_movements, method=\"complete\")\n","\n","# Plot the dendrogram\n","dendrogram(mergings, labels=companies, leaf_rotation=90, leaf_font_size=6)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Different linkage, different hierarchical clustering!\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["samples = pd.read_csv(os.path.join(DATA_PATH, \"eurovision.csv\")).values\n","country_names = [\n","    \"Albania\",\n","    \"Armenia\",\n","    \"Australia\",\n","    \"Austria\",\n","    \"Azerbaijan\",\n","    \"Belarus\",\n","    \"Belgium\",\n","    \"Bosnia & Herzegovina\",\n","    \"Bulgaria\",\n","    \"Croatia\",\n","    \"Cyprus\",\n","    \"Czech Republic\",\n","    \"Denmark\",\n","    \"Estonia\",\n","    \"F.Y.R. Macedonia\",\n","    \"Finland\",\n","    \"France\",\n","    \"Georgia\",\n","    \"Germany\",\n","    \"Greece\",\n","    \"Hungary\",\n","    \"Iceland\",\n","    \"Ireland\",\n","    \"Israel\",\n","    \"Italy\",\n","    \"Latvia\",\n","    \"Lithuania\",\n","    \"Malta\",\n","    \"Moldova\",\n","    \"Montenegro\",\n","    \"Norway\",\n","    \"Poland\",\n","    \"Russia\",\n","    \"San Marino\",\n","    \"Serbia\",\n","    \"Slovenia\",\n","    \"Spain\",\n","    \"Sweden\",\n","    \"Switzerland\",\n","    \"The Netherlands\",\n","    \"Ukraine\",\n","    \"United Kingdom\",\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform the necessary imports\n","import matplotlib.pyplot as plt\n","from scipy.cluster.hierarchy import linkage, dendrogram\n","\n","# Calculate the linkage: mergings\n","mergings = linkage(samples, method=\"single\")\n","\n","# Plot the dendrogram\n","dendrogram(mergings, labels=country_names, leaf_rotation=90, leaf_font_size=6)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Extracting the cluster labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["samples = pd.read_csv(os.path.join(DATA_PATH, \"samples.csv\")).values\n","varieties = [\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Kama wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Rosa wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","    \"Canadian wheat\",\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform the necessary imports\n","import pandas as pd\n","from scipy.cluster.hierarchy import fcluster\n","\n","# Use fcluster to extract labels: labels\n","labels = fcluster(mergings, 6, criterion=\"distance\")\n","\n","# Create a DataFrame with labels and varieties as columns: df\n","df = pd.DataFrame({\"labels\": labels, \"varieties\": varieties})\n","\n","# Create crosstab: ct\n","ct = pd.crosstab(df[\"labels\"], df[\"varieties\"])\n","\n","# Display ct\n","print(ct)\n"]},{"cell_type":"markdown","metadata":{},"source":["### t-SNE visualization of grain dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["variety_numbers = [\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    1,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    2,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","    3,\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import TSNE\n","from sklearn.manifold import TSNE\n","\n","# Create a TSNE instance: model\n","model = TSNE(learning_rate=200)\n","\n","# Apply fit_transform to samples: tsne_features\n","tsne_features = model.fit_transform(samples)\n","\n","# Select the 0th feature: xs\n","xs = tsne_features[:, 0]\n","\n","# Select the 1st feature: ys\n","ys = tsne_features[:, 1]\n","\n","# Scatter plot, coloring by variety_numbers\n","plt.scatter(xs, ys, c=variety_numbers)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### A t-SNE map of the stock market\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import TSNE\n","from sklearn.manifold import TSNE\n","\n","# Create a TSNE instance: model\n","model = TSNE(learning_rate=50)\n","\n","# Apply fit_transform to normalized_movements: tsne_features\n","tsne_features = model.fit_transform(normalized_movements)\n","\n","# Select the 0th feature: xs\n","xs = tsne_features[:, 0]\n","\n","# Select the 1th feature: ys\n","ys = tsne_features[:, 1]\n","\n","# Scatter plot\n","plt.scatter(xs, ys, alpha=0.5)\n","\n","# Annotate the points\n","for x, y, company in zip(xs, ys, companies):\n","    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Decorrelating your data and dimension reduction\n","\n","### Correlated data in nature\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grains = pd.read_csv(os.path.join(DATA_PATH, \"grains.csv\")).values\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform the necessary imports\n","import matplotlib.pyplot as plt\n","from scipy.stats import pearsonr\n","\n","# Assign the 0th column of grains: width\n","width = grains[:, 0]\n","\n","# Assign the 1st column of grains: length\n","length = grains[:, 1]\n","\n","# Scatter plot width vs length\n","plt.scatter(width, length)\n","plt.axis(\"equal\")\n","plt.show()\n","\n","# Calculate the Pearson correlation\n","correlation, pvalue = pearsonr(width, length)\n","\n","# Display the correlation\n","print(correlation)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Decorrelating the grain measurements with PCA\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import PCA\n","from sklearn.decomposition import PCA\n","\n","# Create PCA instance: model\n","model = PCA()\n","\n","# Apply the fit_transform method of model to grains: pca_features\n","pca_features = model.fit_transform(grains)\n","\n","# Assign 0th column of pca_features: xs\n","xs = pca_features[:, 0]\n","\n","# Assign 1st column of pca_features: ys\n","ys = pca_features[:, 1]\n","\n","# Scatter plot xs vs ys\n","plt.scatter(xs, ys)\n","plt.axis(\"equal\")\n","plt.show()\n","\n","# Calculate the Pearson correlation of xs and ys\n","correlation, pvalue = pearsonr(xs, ys)\n","\n","# Display the correlation\n","print(correlation)\n"]},{"cell_type":"markdown","metadata":{},"source":["### The first principal component\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make a scatter plot of the untransformed points\n","plt.scatter(grains[:, 0], grains[:, 1])\n","\n","# Create a PCA instance: model\n","model = PCA(copy=True)\n","\n","# Fit model to points\n","_ = model.fit(grains)\n","\n","# Get the mean of the grain samples: mean\n","mean = model.mean_\n","\n","# Get the first principal component: first_pc\n","first_pc = model.components_[0, :]\n","\n","# Plot first_pc as an arrow, starting at mean\n","plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color=\"red\", width=0.01)\n","\n","# Keep axes on same scale\n","plt.axis(\"equal\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Variance of the PCA features\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["samples = pd.read_csv(os.path.join(DATA_PATH, \"fish.csv\")).values\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform the necessary imports\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","import matplotlib.pyplot as plt\n","\n","# Create scaler: scaler\n","scaler = StandardScaler()\n","\n","# Create a PCA instance: pca\n","pca = PCA(copy=True)\n","\n","# Create pipeline: pipeline\n","pipeline = make_pipeline(scaler, pca)\n","\n","# Fit the pipeline to 'samples'\n","_ = pipeline.fit(samples)\n","\n","# Plot the explained variances\n","features = range(pca.n_components_)\n","plt.bar(features, pca.explained_variance_)\n","plt.xlabel(\"PCA feature\")\n","plt.ylabel(\"variance\")\n","plt.xticks(features)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Dimension reduction of the fish measurements\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scaled_samples = np.array(\n","    [\n","        [-0.50109735, -0.36878558, -0.34323399, -0.23781518, 1.0032125, 0.25373964],\n","        [-0.37434344, -0.29750241, -0.26893461, -0.14634781, 1.15869615, 0.44376493],\n","        [-0.24230812, -0.30641281, -0.25242364, -0.15397009, 1.13926069, 1.0613471],\n","        [-0.18157187, -0.09256329, -0.04603648, 0.02896467, 0.96434159, 0.20623332],\n","        [-0.00464454, -0.0747425, -0.04603648, 0.06707608, 0.8282934, 1.0613471],\n","        [0.04816959, -0.04801131, 0.01175193, 0.12043205, 1.08095432, 0.63379021],\n","        [0.18020491, -0.04801131, 0.01175193, 0.10518748, 1.26559115, 1.15635974],\n","        [-0.11027279, 0.02327186, 0.03651839, 0.14329889, 0.78942248, 0.25373964],\n","        [0.04816959, 0.02327186, 0.03651839, 0.15092117, 1.14897842, 0.44376493],\n","        [0.18020491, 0.10346543, 0.09430679, 0.23476627, 1.09067205, 0.39625861],\n","        [0.11418725, 0.09455503, 0.11907325, 0.23476627, 1.10038978, 0.58628389],\n","        [0.18020491, 0.12128622, 0.11907325, 0.23476627, 1.12954296, 0.20623332],\n","        [0.18020491, 0.1569278, 0.16035069, 0.25001083, 0.94490613, -0.41134885],\n","        [0.44427556, 0.18365899, 0.20162812, 0.31098909, 1.1781316, 0.49127125],\n","        [0.44427556, 0.18365899, 0.20162812, 0.31098909, 1.30446206, 1.01384078],\n","        [0.7083462, 0.27276296, 0.28418298, 0.39483418, 1.04208341, 0.44376493],\n","        [0.7083462, 0.27276296, 0.28418298, 0.41007875, 1.04208341, 0.30124596],\n","        [0.47068262, 0.31731494, 0.32546042, 0.41770103, 1.20728478, 0.20623332],\n","        [0.57631088, 0.32622533, 0.32546042, 0.42532331, 0.90603522, 0.91882813],\n","        [0.3782579, 0.35295652, 0.36673785, 0.48630156, 0.99349477, 0.58628389],\n","        [0.66873561, 0.36186692, 0.36673785, 0.46343472, 1.23643797, 0.39625861],\n","        [0.49708969, 0.37077732, 0.40801528, 0.50154612, 1.07123659, 0.20623332],\n","        [0.65553208, 0.3975085, 0.44929271, 0.57014666, 0.97405931, 1.0613471],\n","        [0.7083462, 0.4064189, 0.44929271, 0.56252438, 1.16841387, 0.44376493],\n","        [0.77436387, 0.3975085, 0.44929271, 0.5930135, 1.15869615, 0.91882813],\n","        [0.76116033, 0.4153293, 0.44929271, 0.57014666, 1.18784933, 1.01384078],\n","        [0.74531609, 0.47770207, 0.53184758, 0.63874719, 1.13926069, 0.58628389],\n","        [1.10445217, 0.48661247, 0.53184758, 0.64636947, 1.21700251, 0.96633446],\n","        [1.50055814, 0.54898524, 0.61440245, 0.72259229, 1.5959939, 1.25137238],\n","        [1.28930162, 0.68264119, 0.73823474, 0.83692651, 1.2461557, 0.68129653],\n","        [1.38172635, 0.68264119, 0.73823474, 0.82930423, 1.26559115, 0.68129653],\n","        [1.30250516, 0.78956594, 0.82078961, 0.92839389, 1.29474434, 0.96633446],\n","        [1.43454048, 0.8964907, 0.94462191, 0.97412758, 1.21700251, 0.87132181],\n","        [1.36852282, 0.94995308, 0.94462191, 1.01986127, 0.95462386, 0.39625861],\n","        [-1.03452005, -1.2865564, -1.27610397, -1.28969003, -0.24065667, 0.53877757],\n","        [-0.95793956, -0.96578213, -0.93762902, -0.97717649, -0.19206803, 0.49127125],\n","        [-0.93417321, -0.87667817, -0.8880961, -0.90857596, -0.17263258, 0.39625861],\n","        [-0.91040685, -0.8143054, -0.80554124, -0.83235314, -0.26980986, 0.68129653],\n","        [-0.82326354, -0.77866381, -0.78903027, -0.83235314, -0.0074312, 1.5364103],\n","        [-1.14014831, -0.74302223, -0.74775283, -0.78661945, 0.03143971, 0.87132181],\n","        [-0.8496706, -0.73411183, -0.72298637, -0.76375261, -0.13376167, 0.87132181],\n","        [-0.82326354, -0.70738064, -0.7064754, -0.71801892, -0.22122122, 0.49127125],\n","        [-0.74404234, -0.61827668, -0.62392054, -0.6417961, -0.44472896, 1.10885342],\n","        [-0.75724587, -0.60936628, -0.62392054, -0.67228523, -0.0754553, 0.82381549],\n","        [-0.71763528, -0.60936628, -0.5826431, -0.59606241, -0.02686666, 1.0613471],\n","        [-0.77044941, -0.5648143, -0.5826431, -0.61892926, -0.18235031, 0.20623332],\n","        [-0.71763528, -0.5559039, -0.5826431, -0.61892926, -0.24065667, 1.10885342],\n","        [-0.69386892, -0.47571034, -0.4588108, -0.45123907, -0.03658439, 0.58628389],\n","        [-0.71499457, -0.47571034, -0.50834372, -0.48935047, -0.21150349, 0.34875228],\n","        [-0.61200702, -0.46679994, -0.50008824, -0.48172819, -0.04630212, 1.20386606],\n","        [-0.66482115, -0.33314399, -0.35974497, -0.39788309, -0.26009213, 0.53877757],\n","        [-0.37434344, -0.29750241, -0.29370107, -0.29879344, 0.22579427, 1.20386606],\n","        [-0.42187616, -0.20839845, -0.21114621, -0.19208149, -0.0074312, 1.2988787],\n","        [-0.11027279, 0.19256939, 0.17686166, 0.14329889, -0.09489075, 1.15635974],\n","        [-1.12245558, -1.60733067, -1.63108989, -1.70129323, -1.16384082, -1.50399423],\n","        [-1.12034301, -1.5449579, -1.57330149, -1.64031498, -1.07638127, -1.36147526],\n","        [-1.12166336, -1.5360475, -1.565046, -1.64031498, -1.28045356, -1.40898159],\n","        [-1.11453346, -1.50931631, -1.53202405, -1.60982586, -0.95005081, -0.64888045],\n","        [-1.11426939, -1.48258512, -1.51551308, -1.57933673, -1.09581673, -1.2189563],\n","        [-1.11717416, -1.47367473, -1.5072576, -1.56409217, -1.20271174, -1.26646262],\n","        [-1.11374125, -1.42912274, -1.46598016, -1.52598076, -1.086099, -1.45648791],\n","        [-1.11400532, -1.42912274, -1.46598016, -1.52598076, -1.086099, -1.88404479],\n","        [-1.11426939, -1.42021235, -1.44946919, -1.51835848, -1.10553446, -1.97905744],\n","        [-1.10793169, -1.41130195, -1.43295822, -1.50311391, -1.21242946, -1.17144998],\n","        [-1.10476284, -1.39348116, -1.41644724, -1.49549163, -0.97920399, -1.64651319],\n","        [-1.10793169, -1.35783957, -1.36691432, -1.47262479, -1.12496991, -1.78903215],\n","        [-1.08812639, -1.25982521, -1.259593, -1.36591285, -0.89174444, 0.34875228],\n","        [-1.08759825, -1.20636284, -1.20180459, -1.28969003, -0.96948627, -0.60137413],\n","        [-0.61200702, 0.23712137, 0.22639458, 0.12805433, -1.17355855, -1.50399423],\n","        [-0.34793638, 0.38859811, 0.36673785, 0.35672277, -1.2610181, -0.88641206],\n","        [-0.34793638, 0.47770207, 0.44929271, 0.43294559, -1.24158265, -0.74389309],\n","        [-0.34793638, 0.6648204, 0.6391689, 0.5091684, -1.19299401, -1.31396894],\n","        [-0.00464454, 0.72719317, 0.69695731, 0.56252438, -0.97920399, -0.74389309],\n","        [-0.22910458, 0.77174515, 0.73823474, 0.60063578, -1.21242946, -1.50399423],\n","        [0.06401383, 1.128161, 1.0684542, 0.94363845, -1.17355855, -1.59900687],\n","        [0.20661198, 1.128161, 1.0684542, 0.94363845, -1.27073583, -1.45648791],\n","        [0.28583317, 1.1370714, 1.10973164, 0.9665053, -1.07638127, -0.79139942],\n","        [0.18020491, 1.30636893, 1.27484137, 1.13419549, -1.31932447, -1.26646262],\n","        [0.35713225, 1.41329369, 1.35739623, 1.18755146, -1.17355855, -1.36147526],\n","        [0.89319566, 1.55586003, 1.52250596, 1.3781085, -1.27073583, -1.12394366],\n","        [1.36852282, 1.8677239, 1.82795897, 1.67537748, -1.1541231, -0.79139942],\n","        [2.16073475, 2.19740857, 2.18294489, 2.02600243, -0.98892172, -0.55386781],\n","        [3.08498201, 2.55382442, 2.51316435, 2.35376053, -1.27073583, -1.55150055],\n","        [2.95294669, 2.55382442, 2.51316435, 2.35376053, -1.27073583, -1.55150055],\n","        [3.21701733, 2.82113631, 2.79385089, 2.65865179, -1.18327628, -0.88641206],\n","    ]\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import PCA\n","from sklearn.decomposition import PCA\n","\n","# Create a PCA model with 2 components: pca\n","pca = PCA(n_components=2)\n","\n","# Fit the PCA instance to the scaled samples\n","_ = pca.fit(scaled_samples)\n","\n","# Transform the scaled samples: pca_features\n","pca_features = pca.transform(scaled_samples)\n","\n","# Print the shape of pca_features\n","print(pca_features.shape)\n"]},{"cell_type":"markdown","metadata":{},"source":["### A tf-idf word-frequency array\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["documents = [\"cats say meow\", \"dogs say woof\", \"dogs chase cats\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Create a TfidfVectorizer: tfidf\n","tfidf = TfidfVectorizer()\n","\n","# Apply fit_transform to document: csr_mat\n","csr_mat = tfidf.fit_transform(documents)\n","\n","# Print result of toarray() method\n","print(csr_mat.toarray())\n","\n","# Get the words: words\n","words = tfidf.get_feature_names()\n","\n","# Print words\n","print(words)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Clustering Wikipedia part I\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform the necessary imports\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.cluster import KMeans\n","from sklearn.pipeline import make_pipeline\n","\n","# Create a TruncatedSVD instance: svd\n","svd = TruncatedSVD(n_components=50)\n","\n","# Create a KMeans instance: kmeans\n","kmeans = KMeans(n_clusters=6)\n","\n","# Create a pipeline: pipeline\n","pipeline = make_pipeline(svd, kmeans)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Clustering Wikipedia part II\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from scipy.sparse import csr_matrix\n","\n","# articles = csr_matrix(np.fromfile(os.path.join(DATA_PATH, \"articles.csv\"), sep=\",\"))\n","# titles = [\n","#     \"HTTP 404\",\n","#     \"Alexa Internet\",\n","#     \"Internet Explorer\",\n","#     \"HTTP cookie\",\n","#     \"Google Search\",\n","#     \"Tumblr\",\n","#     \"Hypertext Transfer Protocol\",\n","#     \"Social search\",\n","#     \"Firefox\",\n","#     \"LinkedIn\",\n","#     \"Global warming\",\n","#     \"Nationally Appropriate Mitigation Action\",\n","#     \"Nigel Lawson\",\n","#     \"Connie Hedegaard\",\n","#     \"Climate change\",\n","#     \"Kyoto Protocol\",\n","#     \"350.org\",\n","#     \"Greenhouse gas emissions by the United States\",\n","#     \"2010 United Nations Climate Change Conference\",\n","#     \"2007 United Nations Climate Change Conference\",\n","#     \"Angelina Jolie\",\n","#     \"Michael Fassbender\",\n","#     \"Denzel Washington\",\n","#     \"Catherine Zeta-Jones\",\n","#     \"Jessica Biel\",\n","#     \"Russell Crowe\",\n","#     \"Mila Kunis\",\n","#     \"Dakota Fanning\",\n","#     \"Anne Hathaway\",\n","#     \"Jennifer Aniston\",\n","#     \"France national football team\",\n","#     \"Cristiano Ronaldo\",\n","#     \"Arsenal F.C.\",\n","#     \"Radamel Falcao\",\n","#     \"Zlatan Ibrahimović\",\n","#     \"Colombia national football team\",\n","#     \"2014 FIFA World Cup qualification\",\n","#     \"Football\",\n","#     \"Neymar\",\n","#     \"Franck Ribéry\",\n","#     \"Tonsillitis\",\n","#     \"Hepatitis B\",\n","#     \"Doxycycline\",\n","#     \"Leukemia\",\n","#     \"Gout\",\n","#     \"Hepatitis C\",\n","#     \"Prednisone\",\n","#     \"Fever\",\n","#     \"Gabapentin\",\n","#     \"Lymphoma\",\n","#     \"Chad Kroeger\",\n","#     \"Nate Ruess\",\n","#     \"The Wanted\",\n","#     \"Stevie Nicks\",\n","#     \"Arctic Monkeys\",\n","#     \"Black Sabbath\",\n","#     \"Skrillex\",\n","#     \"Red Hot Chili Peppers\",\n","#     \"Sepsis\",\n","#     \"Adam Levine\",\n","# ]\n"]},{"cell_type":"markdown","metadata":{},"source":["```python\n","# Import pandas\n","import pandas as pd\n","\n","# Fit the pipeline to articles\n","_ = pipeline.fit(articles)\n","\n","# Calculate the cluster labels: labels\n","labels = pipeline.predict(articles)\n","\n","# Create a DataFrame aligning labels and titles: df\n","df = pd.DataFrame({\"label\": labels, \"article\": titles})\n","\n","# Display df sorted by cluster label\n","print(df.sort_values(\"label\"))\n","\n","```"]},{"cell_type":"markdown","metadata":{},"source":["---\n","\n","## Discovering interpretable features\n","\n","### NMF applied to Wikipedia articles"]},{"cell_type":"markdown","metadata":{},"source":["```python\n","# Import NMF\n","from sklearn.decomposition import NMF\n","\n","# Create an NMF instance: model\n","model = NMF(n_components=6)\n","\n","# Fit the model to articles\n","_ = model.fit(articles)\n","\n","# Transform the articles: nmf_features\n","nmf_features = model.transform(articles)\n","\n","# Print the NMF features\n","print(nmf_features.round(2))\n","\n","```"]},{"cell_type":"markdown","metadata":{},"source":["### NMF features of the Wikipedia articles\n"]},{"cell_type":"markdown","metadata":{},"source":["```python\n","# Import pandas\n","import pandas as pd\n","\n","# Create a pandas DataFrame: df\n","df = pd.DataFrame(nmf_features, index=titles)\n","\n","# Print the row for 'Anne Hathaway'\n","print(df.loc[\"Anne Hathaway\"])\n","\n","# Print the row for 'Denzel Washington'\n","print(df.loc[\"Denzel Washington\"])\n","\n","```"]},{"cell_type":"markdown","metadata":{},"source":["### NMF learns topics of documents\n"]},{"cell_type":"markdown","metadata":{},"source":["```python\n","# Import pandas\n","import pandas as pd\n","\n","# Create a DataFrame: components_df\n","components_df = pd.DataFrame(model.components_, columns=words)\n","\n","# Print the shape of the DataFrame\n","print(components_df.shape)\n","\n","# Select row 3: component\n","component = components_df.iloc[3]\n","\n","# Print result of nlargest\n","print(component.nlargest())\n","\n","```"]},{"cell_type":"markdown","metadata":{},"source":["### Explore the LED digits dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["samples = pd.read_csv(os.path.join(DATA_PATH, \"led.csv\")).values\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import pyplot\n","from matplotlib import pyplot as plt\n","\n","# Select the 0th row: digit\n","digit = samples[0]\n","\n","# Print digit\n","print(digit)\n","\n","# Reshape digit to a 13x8 array: bitmap\n","bitmap = digit.reshape((13, 8))\n","\n","# Print bitmap\n","print(bitmap)\n","\n","# Use plt.imshow to display bitmap\n","plt.imshow(bitmap, cmap=\"gray\", interpolation=\"nearest\")\n","plt.colorbar()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### NMF learns the parts of images\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def show_as_image(sample):\n","    bitmap = sample.reshape((13, 8))\n","    plt.figure()\n","    plt.imshow(bitmap, cmap=\"gray\", interpolation=\"nearest\")\n","    plt.colorbar()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import NMF\n","from sklearn.decomposition import NMF\n","\n","# Create an NMF model: model\n","model = NMF(n_components=7)\n","\n","# Apply fit_transform to samples: features\n","features = model.fit_transform(samples)\n","\n","# Call show_as_image on each component\n","for component in model.components_:\n","    show_as_image(component)\n","\n","# Assign the 0th row of features: digit_features\n","digit_features = features[0]\n","\n","# Print digit_features\n","print(digit_features)\n"]},{"cell_type":"markdown","metadata":{},"source":["### PCA doesn't learn parts\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import PCA\n","from sklearn.decomposition import PCA\n","\n","# Create a PCA instance: model\n","model = PCA(n_components=7)\n","\n","# Apply fit_transform to samples: features\n","features = model.fit_transform(samples)\n","\n","# Call show_as_image on each component\n","for component in model.components_:\n","    show_as_image(component)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Which articles are similar to 'Cristiano Ronaldo'?\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nmf_features = pd.read_csv(os.path.join(DATA_PATH, \"nmf.csv\"), header=None).values\n","titles = [\n","    \"HTTP 404\",\n","    \"Alexa Internet\",\n","    \"Internet Explorer\",\n","    \"HTTP cookie\",\n","    \"Google Search\",\n","    \"Tumblr\",\n","    \"Hypertext Transfer Protocol\",\n","    \"Social search\",\n","    \"Firefox\",\n","    \"LinkedIn\",\n","    \"Global warming\",\n","    \"Nationally Appropriate Mitigation Action\",\n","    \"Nigel Lawson\",\n","    \"Connie Hedegaard\",\n","    \"Climate change\",\n","    \"Kyoto Protocol\",\n","    \"350.org\",\n","    \"Greenhouse gas emissions by the United States\",\n","    \"2010 United Nations Climate Change Conference\",\n","    \"2007 United Nations Climate Change Conference\",\n","    \"Angelina Jolie\",\n","    \"Michael Fassbender\",\n","    \"Denzel Washington\",\n","    \"Catherine Zeta-Jones\",\n","    \"Jessica Biel\",\n","    \"Russell Crowe\",\n","    \"Mila Kunis\",\n","    \"Dakota Fanning\",\n","    \"Anne Hathaway\",\n","    \"Jennifer Aniston\",\n","    \"France national football team\",\n","    \"Cristiano Ronaldo\",\n","    \"Arsenal F.C.\",\n","    \"Radamel Falcao\",\n","    \"Zlatan Ibrahimović\",\n","    \"Colombia national football team\",\n","    \"2014 FIFA World Cup qualification\",\n","    \"Football\",\n","    \"Neymar\",\n","    \"Franck Ribéry\",\n","    \"Tonsillitis\",\n","    \"Hepatitis B\",\n","    \"Doxycycline\",\n","    \"Leukemia\",\n","    \"Gout\",\n","    \"Hepatitis C\",\n","    \"Prednisone\",\n","    \"Fever\",\n","    \"Gabapentin\",\n","    \"Lymphoma\",\n","    \"Chad Kroeger\",\n","    \"Nate Ruess\",\n","    \"The Wanted\",\n","    \"Stevie Nicks\",\n","    \"Arctic Monkeys\",\n","    \"Black Sabbath\",\n","    \"Skrillex\",\n","    \"Red Hot Chili Peppers\",\n","    \"Sepsis\",\n","    \"Adam Levine\",\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform the necessary imports\n","import pandas as pd\n","from sklearn.preprocessing import normalize\n","\n","# Normalize the NMF features: norm_features\n","norm_features = normalize(nmf_features)\n","\n","# Create a DataFrame: df\n","df = pd.DataFrame(norm_features, index=titles)\n","\n","# Select the row corresponding to 'Cristiano Ronaldo': article\n","article = df.loc[\"Cristiano Ronaldo\"]\n","\n","# Compute the dot products: similarities\n","similarities = df.dot(article)\n","\n","# Display those with the largest cosine similarity\n","print(similarities.nlargest())\n"]},{"cell_type":"markdown","metadata":{},"source":["### Recommend musical artists part I\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.sparse import csr_matrix\n","\n","artists = csr_matrix(\n","    np.asmatrix(pd.read_csv(os.path.join(DATA_PATH, \"artists.csv\"), header=None).values)\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Perform the necessary imports\n","from sklearn.decomposition import NMF\n","from sklearn.preprocessing import Normalizer, MaxAbsScaler\n","from sklearn.pipeline import make_pipeline\n","\n","# Create a MaxAbsScaler: scaler\n","scaler = MaxAbsScaler()\n","\n","# Create an NMF model: nmf\n","nmf = NMF(n_components=20)\n","\n","# Create a Normalizer: normalizer\n","normalizer = Normalizer()\n","\n","# Create a pipeline: pipeline\n","pipeline = make_pipeline(scaler, nmf, normalizer)\n","\n","# Apply fit_transform to artists: norm_features\n","norm_features = pipeline.fit_transform(artists)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Recommend musical artists part II\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["artist_names = [\n","    \"Massive Attack\",\n","    \"Sublime\",\n","    \"Beastie Boys\",\n","    \"Neil Young\",\n","    \"Dead Kennedys\",\n","    \"Orbital\",\n","    \"Miles Davis\",\n","    \"Leonard Cohen\",\n","    \"Van Morrison\",\n","    \"NOFX\",\n","    \"Rancid\",\n","    \"Lamb\",\n","    \"Korn\",\n","    \"Dropkick Murphys\",\n","    \"Bob Dylan\",\n","    \"Eminem\",\n","    \"Nirvana\",\n","    \"Van Halen\",\n","    \"Damien Rice\",\n","    \"Elvis Costello\",\n","    \"Everclear\",\n","    \"Jimi Hendrix\",\n","    \"PJ Harvey\",\n","    \"Red Hot Chili Peppers\",\n","    \"Ryan Adams\",\n","    \"Soundgarden\",\n","    \"The White Stripes\",\n","    \"Madonna\",\n","    \"Eric Clapton\",\n","    \"Bob Marley\",\n","    \"Dr. Dre\",\n","    \"The Flaming Lips\",\n","    \"Tom Waits\",\n","    \"Moby\",\n","    \"Cypress Hill\",\n","    \"Garbage\",\n","    \"Fear Factory\",\n","    \"50 Cent\",\n","    \"Ani DiFranco\",\n","    \"Matchbox Twenty\",\n","    \"The Police\",\n","    \"Eagles\",\n","    \"Phish\",\n","    \"Stone Temple Pilots\",\n","    \"Black Sabbath\",\n","    \"Britney Spears\",\n","    \"Fatboy Slim\",\n","    \"System of a Down\",\n","    \"Simon & Garfunkel\",\n","    \"Snoop Dogg\",\n","    \"Aimee Mann\",\n","    \"Less Than Jake\",\n","    \"Rammstein\",\n","    \"Reel Big Fish\",\n","    \"The Prodigy\",\n","    \"Pantera\",\n","    \"Foo Fighters\",\n","    \"The Beatles\",\n","    \"Incubus\",\n","    \"Audioslave\",\n","    \"Bright Eyes\",\n","    \"Machine Head\",\n","    \"AC/DC\",\n","    \"Dire Straits\",\n","    \"MotÃ¶rhead\",\n","    \"Ramones\",\n","    \"Slipknot\",\n","    \"Me First and the Gimme Gimmes\",\n","    \"Bruce Springsteen\",\n","    \"Queens of the Stone Age\",\n","    \"The Chemical Brothers\",\n","    \"Bon Jovi\",\n","    \"Goo Goo Dolls\",\n","    \"Alice in Chains\",\n","    \"Howard Shore\",\n","    \"Barenaked Ladies\",\n","    \"Anti-Flag\",\n","    \"Nick Cave and the Bad Seeds\",\n","    \"Static-X\",\n","    \"Misfits\",\n","    \"2Pac\",\n","    \"Sparta\",\n","    \"Interpol\",\n","    \"The Crystal Method\",\n","    \"The Beach Boys\",\n","    \"Goldfrapp\",\n","    \"Bob Marley & the Wailers\",\n","    \"Kylie Minogue\",\n","    \"The Blood Brothers\",\n","    \"Mirah\",\n","    \"Ludacris\",\n","    \"Snow Patrol\",\n","    \"The Mars Volta\",\n","    \"Yeah Yeah Yeahs\",\n","    \"Iced Earth\",\n","    \"Fiona Apple\",\n","    \"Rilo Kiley\",\n","    \"Rufus Wainwright\",\n","    \"Flogging Molly\",\n","    \"Hot Hot Heat\",\n","    \"Dredg\",\n","    \"Switchfoot\",\n","    \"Tegan and Sara\",\n","    \"Rage Against the Machine\",\n","    \"Keane\",\n","    \"Jet\",\n","    \"Franz Ferdinand\",\n","    \"The Postal Service\",\n","    \"The Dresden Dolls\",\n","    \"The Killers\",\n","    \"Death From Above 1979\",\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import pandas\n","import pandas as pd\n","\n","# Create a DataFrame: df\n","df = pd.DataFrame(norm_features, index=artist_names)\n","\n","# Select row of 'Bruce Springsteen': artist\n","artist = df.loc[\"Bruce Springsteen\"]\n","\n","# Compute cosine similarities: similarities\n","similarities = df.dot(artist)\n","\n","# Display those with highest cosine similarity\n","print(similarities.nlargest())\n"]}],"metadata":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
