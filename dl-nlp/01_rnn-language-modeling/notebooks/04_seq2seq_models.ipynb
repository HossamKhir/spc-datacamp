{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence `seq2seq`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model, model_from_json\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "\n",
    "DATA_PATH = \"../data/raw\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-09 17:05:39.879602: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-03-09 17:05:39.879652: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: optimus\n",
      "2022-03-09 17:05:39.879667: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: optimus\n",
      "2022-03-09 17:05:39.879783: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.42.1\n",
      "2022-03-09 17:05:39.879812: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.42.1\n",
      "2022-03-09 17:05:39.879819: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.42.1\n",
      "2022-03-09 17:05:39.880097: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "Tx = 40\n",
    "index_to_char = {\n",
    "    0: \" \",\n",
    "    1: \"!\",\n",
    "    2: \"$\",\n",
    "    3: \"%\",\n",
    "    4: \"&\",\n",
    "    5: \"(\",\n",
    "    6: \")\",\n",
    "    7: \",\",\n",
    "    8: \"-\",\n",
    "    9: \".\",\n",
    "    10: \"/\",\n",
    "    11: \"0\",\n",
    "    12: \"1\",\n",
    "    13: \"2\",\n",
    "    14: \"3\",\n",
    "    15: \"4\",\n",
    "    16: \"5\",\n",
    "    17: \"6\",\n",
    "    18: \"7\",\n",
    "    19: \"8\",\n",
    "    20: \"9\",\n",
    "    21: \";\",\n",
    "    22: \"<\",\n",
    "    23: \"=\",\n",
    "    24: \">\",\n",
    "    25: \"?\",\n",
    "    26: \"a\",\n",
    "    27: \"b\",\n",
    "    28: \"c\",\n",
    "    29: \"d\",\n",
    "    30: \"e\",\n",
    "    31: \"f\",\n",
    "    32: \"g\",\n",
    "    33: \"h\",\n",
    "    34: \"i\",\n",
    "    35: \"j\",\n",
    "    36: \"k\",\n",
    "    37: \"l\",\n",
    "    38: \"m\",\n",
    "    39: \"n\",\n",
    "    40: \"o\",\n",
    "    41: \"p\",\n",
    "    42: \"q\",\n",
    "    43: \"r\",\n",
    "    44: \"s\",\n",
    "    45: \"t\",\n",
    "    46: \"u\",\n",
    "    47: \"v\",\n",
    "    48: \"w\",\n",
    "    49: \"x\",\n",
    "    50: \"y\",\n",
    "    51: \"z\",\n",
    "    52: \"}\",\n",
    "    53: \"¿\",\n",
    "    54: \"à\",\n",
    "    55: \"á\",\n",
    "    56: \"è\",\n",
    "    57: \"é\",\n",
    "    58: \"ê\",\n",
    "    59: \"ñ\",\n",
    "    60: \"ó\",\n",
    "    61: \"ö\",\n",
    "    62: \"ü\",\n",
    "    63: \"–\",\n",
    "    64: \"‘\",\n",
    "    65: \"’\",\n",
    "    66: \"“\",\n",
    "    67: \"”\",\n",
    "    68: \"…\",\n",
    "}\n",
    "char_to_index = {\n",
    "    \" \": 0,\n",
    "    \"!\": 1,\n",
    "    \"$\": 2,\n",
    "    \"%\": 3,\n",
    "    \"&\": 4,\n",
    "    \"(\": 5,\n",
    "    \")\": 6,\n",
    "    \",\": 7,\n",
    "    \"-\": 8,\n",
    "    \".\": 9,\n",
    "    \"/\": 10,\n",
    "    \"0\": 11,\n",
    "    \"1\": 12,\n",
    "    \"2\": 13,\n",
    "    \"3\": 14,\n",
    "    \"4\": 15,\n",
    "    \"5\": 16,\n",
    "    \"6\": 17,\n",
    "    \"7\": 18,\n",
    "    \"8\": 19,\n",
    "    \"9\": 20,\n",
    "    \";\": 21,\n",
    "    \"<\": 22,\n",
    "    \"=\": 23,\n",
    "    \">\": 24,\n",
    "    \"?\": 25,\n",
    "    \"a\": 26,\n",
    "    \"b\": 27,\n",
    "    \"c\": 28,\n",
    "    \"d\": 29,\n",
    "    \"e\": 30,\n",
    "    \"f\": 31,\n",
    "    \"g\": 32,\n",
    "    \"h\": 33,\n",
    "    \"i\": 34,\n",
    "    \"j\": 35,\n",
    "    \"k\": 36,\n",
    "    \"l\": 37,\n",
    "    \"m\": 38,\n",
    "    \"n\": 39,\n",
    "    \"o\": 40,\n",
    "    \"p\": 41,\n",
    "    \"q\": 42,\n",
    "    \"r\": 43,\n",
    "    \"s\": 44,\n",
    "    \"t\": 45,\n",
    "    \"u\": 46,\n",
    "    \"v\": 47,\n",
    "    \"w\": 48,\n",
    "    \"x\": 49,\n",
    "    \"y\": 50,\n",
    "    \"z\": 51,\n",
    "    \"}\": 52,\n",
    "    \"¿\": 53,\n",
    "    \"à\": 54,\n",
    "    \"á\": 55,\n",
    "    \"è\": 56,\n",
    "    \"é\": 57,\n",
    "    \"ê\": 58,\n",
    "    \"ñ\": 59,\n",
    "    \"ó\": 60,\n",
    "    \"ö\": 61,\n",
    "    \"ü\": 62,\n",
    "    \"–\": 63,\n",
    "    \"‘\": 64,\n",
    "    \"’\": 65,\n",
    "    \"“\": 66,\n",
    "    \"”\": 67,\n",
    "    \"…\": 68,\n",
    "}\n",
    "poem_chars = [\n",
    "    \"\\n\",\n",
    "    \" \",\n",
    "    \"!\",\n",
    "    \"'\",\n",
    "    \"(\",\n",
    "    \")\",\n",
    "    \",\",\n",
    "    \"-\",\n",
    "    \".\",\n",
    "    \":\",\n",
    "    \";\",\n",
    "    \"?\",\n",
    "    \"a\",\n",
    "    \"b\",\n",
    "    \"c\",\n",
    "    \"d\",\n",
    "    \"e\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    \"h\",\n",
    "    \"i\",\n",
    "    \"j\",\n",
    "    \"k\",\n",
    "    \"l\",\n",
    "    \"m\",\n",
    "    \"n\",\n",
    "    \"o\",\n",
    "    \"p\",\n",
    "    \"q\",\n",
    "    \"r\",\n",
    "    \"s\",\n",
    "    \"t\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"w\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "]\n",
    "poem_char_to_index = {\n",
    "    \"\\n\": 0,\n",
    "    \" \": 1,\n",
    "    \"!\": 2,\n",
    "    \"'\": 3,\n",
    "    \"(\": 4,\n",
    "    \")\": 5,\n",
    "    \",\": 6,\n",
    "    \"-\": 7,\n",
    "    \".\": 8,\n",
    "    \":\": 9,\n",
    "    \";\": 10,\n",
    "    \"?\": 11,\n",
    "    \"a\": 12,\n",
    "    \"b\": 13,\n",
    "    \"c\": 14,\n",
    "    \"d\": 15,\n",
    "    \"e\": 16,\n",
    "    \"f\": 17,\n",
    "    \"g\": 18,\n",
    "    \"h\": 19,\n",
    "    \"i\": 20,\n",
    "    \"j\": 21,\n",
    "    \"k\": 22,\n",
    "    \"l\": 23,\n",
    "    \"m\": 24,\n",
    "    \"n\": 25,\n",
    "    \"o\": 26,\n",
    "    \"p\": 27,\n",
    "    \"q\": 28,\n",
    "    \"r\": 29,\n",
    "    \"s\": 30,\n",
    "    \"t\": 31,\n",
    "    \"u\": 32,\n",
    "    \"v\": 33,\n",
    "    \"w\": 34,\n",
    "    \"x\": 35,\n",
    "    \"y\": 36,\n",
    "    \"z\": 37,\n",
    "}\n",
    "poem_index_to_char = {\n",
    "    0: \"\\n\",\n",
    "    1: \" \",\n",
    "    2: \"!\",\n",
    "    3: \"'\",\n",
    "    4: \"(\",\n",
    "    5: \")\",\n",
    "    6: \",\",\n",
    "    7: \"-\",\n",
    "    8: \".\",\n",
    "    9: \":\",\n",
    "    10: \";\",\n",
    "    11: \"?\",\n",
    "    12: \"a\",\n",
    "    13: \"b\",\n",
    "    14: \"c\",\n",
    "    15: \"d\",\n",
    "    16: \"e\",\n",
    "    17: \"f\",\n",
    "    18: \"g\",\n",
    "    19: \"h\",\n",
    "    20: \"i\",\n",
    "    21: \"j\",\n",
    "    22: \"k\",\n",
    "    23: \"l\",\n",
    "    24: \"m\",\n",
    "    25: \"n\",\n",
    "    26: \"o\",\n",
    "    27: \"p\",\n",
    "    28: \"q\",\n",
    "    29: \"r\",\n",
    "    30: \"s\",\n",
    "    31: \"t\",\n",
    "    32: \"u\",\n",
    "    33: \"v\",\n",
    "    34: \"w\",\n",
    "    35: \"x\",\n",
    "    36: \"y\",\n",
    "    37: \"z\",\n",
    "}\n",
    "\n",
    "poem_model = load_model(os.path.join(DATA_PATH, \"poem_model.h5\"))\n",
    "sheldon_model = json.load(open(os.path.join(DATA_PATH, \"models/sheldon_model.json\")))\n",
    "sheldon_model = model_from_json(json.dumps(sheldon_model))\n",
    "sheldon_model.load_weights(os.path.join(DATA_PATH, \"weights/sheldon_model_weights.h5\"))\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def generate_sheldon_phrase(model, initial_text=None):\n",
    "    \"\"\"Generate a phrase based on trained model with Sheldon's sentences.\n",
    "\n",
    "    Use the trained model to generate a full sentence.\n",
    "    The sentence is limited to 200 characters and is defined as a sequence of characters until a '.' is generated.\n",
    "\n",
    "    Args:\n",
    "        model (keras.models.Sequential): The pre-trained model.\n",
    "        initial_text (string): A context string for the model (defaults to None).\n",
    "\n",
    "    Returns:\n",
    "        A generated phrase (string).\n",
    "    \"\"\"\n",
    "\n",
    "    # Define default parameters\n",
    "    chars_window = 20\n",
    "    n_vocab = 69\n",
    "    ix_to_word = index_to_char\n",
    "    word_to_ix = char_to_index\n",
    "    limit = 200\n",
    "    temperature = 0.25\n",
    "    sheldon_quotes = [\n",
    "        \"You're afraid of insects and women, Ladybugs must render you catatonic.\",\n",
    "        \"Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.\",\n",
    "        \"For example, I cry because others are stupid, and that makes me sad.\",\n",
    "        \"I’m not insane, my mother had me tested.\",\n",
    "        \"Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.\",\n",
    "        \"Amy’s birthday present will be my genitals.\",\n",
    "        \"(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!\",\n",
    "        \"Thankfully all the things my girlfriend used to do can be taken care of with my right hand.\",\n",
    "        \"I would have been here sooner but the bus kept stopping for other people to get on it.\",\n",
    "        \"Oh gravity, thou art a heartless bitch.\",\n",
    "        \"I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.\",\n",
    "        \"Well, today we tried masturbating for money.\",\n",
    "        \"I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.\",\n",
    "        \"Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.\",\n",
    "        \"What computer do you have? And please don't say a white one.\",\n",
    "        \"She calls me moon-pie because I'm nummy-nummy and she could just eat me up.\",\n",
    "        \"Ah, memory impairment; the free prize at the bottom of every vodka bottle.\",\n",
    "    ]\n",
    "    if initial_text is None:\n",
    "        # initial_text = np.random.choice(sheldon_quotes)\n",
    "        initial_text = sheldon_quotes[3]\n",
    "    res = \"\"\n",
    "    seq = initial_text.lower()\n",
    "    res += seq\n",
    "    length_gap = chars_window - len(initial_text)\n",
    "    if length_gap >= 0:\n",
    "        seq = \" \" * length_gap + seq\n",
    "    else:\n",
    "        seq = seq[:length_gap]\n",
    "        # print('Initial text too long, padded to: {0}'.format(seq))\n",
    "    counter = 0\n",
    "    word = \"\"\n",
    "    while counter < limit and word != r\".\":\n",
    "        x_pred = np.zeros((1, chars_window, n_vocab))\n",
    "        for t, char in enumerate(seq):\n",
    "            x_pred[0, t, word_to_ix[char]] = 1.0\n",
    "        pred = model.predict(x_pred, verbose=0)[0]\n",
    "        # idx = np.argmax(pred)\n",
    "        idx = sample(pred, temperature)\n",
    "        word = ix_to_word[idx]\n",
    "        # print(word)\n",
    "        res += word\n",
    "        seq = seq[1:] + word\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def generate_poem(model, initial_text):\n",
    "    \"\"\"Generate a poem using the initial text and the trained model.\n",
    "\n",
    "    Use the trained model to generate a poem based on the trained model.\n",
    "    The model was trained using the Sonnets from Shakespeare. It will\n",
    "    generate a poem limited to 400 characters.\n",
    "\n",
    "    Args:\n",
    "        model (keras.models.Sequential): The pre-trained model.\n",
    "        initial_text (string): A context string for the model (defaults to None).\n",
    "\n",
    "    Returns:\n",
    "        A generated phrase (string).\n",
    "    \"\"\"\n",
    "\n",
    "    # Define some parameters\n",
    "    limit = 400\n",
    "    temperature = 0.5\n",
    "\n",
    "    generated = \"\"\n",
    "    # zero pad the sentence to Tx characters.\n",
    "    sentence = (\"{0:0>\" + str(Tx) + \"}\").format(initial_text).lower()\n",
    "    generated += initial_text\n",
    "\n",
    "    sent_count = 0\n",
    "\n",
    "    print(\"\\n\\nYour poem lies below: \\n\\n\")\n",
    "    for _ in range(limit):\n",
    "\n",
    "        x_pred = np.zeros((1, Tx, len(poem_chars)))\n",
    "\n",
    "        for t, char in enumerate(sentence):\n",
    "            if char != \"0\":\n",
    "                x_pred[0, t, poem_char_to_index[char]] = 1.0\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=temperature)\n",
    "        next_char = poem_index_to_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        if next_char == \"\\n\":\n",
    "            sent_count += 1\n",
    "            if sent_count > 8:\n",
    "                break\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_450118/2823052963.py:283: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.log(preds) / temperature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i’m not insane, my mother had me tested. ovie with the relation.\n",
      "\n",
      "\n",
      "Your poem lies below: \n",
      "\n",
      "\n",
      "May thy beauty forever remain,\n",
      "without thy shall i semart that might beautes lever\n",
      "for thy wall of your betury me.t the seentich frow,\n",
      "thy she thou lives and partst all distauty deet,\n",
      "nor the brave whe form of formes's lays,\n",
      "with nets and prays and from my forses his brand,\n",
      "in the well which chuch mest i hay which now,\n",
      "and from the world and sil mor the his prous ray,\n",
      "and sull reeps with from macks of the wart,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Context for Sheldon phrase\n",
    "sheldon_context = \"I’m not insane, my mother had me tested. \"\n",
    "\n",
    "# Generate one Sheldon phrase\n",
    "sheldon_phrase = generate_sheldon_phrase(sheldon_model, sheldon_context)\n",
    "\n",
    "# Print the phrase\n",
    "print(sheldon_phrase)\n",
    "\n",
    "# Context for poem\n",
    "poem_context = \"May thy beauty forever remain\"\n",
    "\n",
    "# Print the poem\n",
    "print(generate_poem(poem_model, poem_context))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
    "\n",
    "pt_length = 8\n",
    "pt_tokenizer = pickle.load(open(os.path.join(DATA_PATH, \"pt_tokenizer.pickle\"), \"rb\"))\n",
    "index_to_word = pickle.load(open(os.path.join(DATA_PATH, \"index_to_word.pkl\"), \"rb\"))\n",
    "\n",
    "sentences = [\n",
    "    \"obrigada\",\n",
    "    \"vocês podem ficar\",\n",
    "    \"ataque\",\n",
    "    \"eu acredito em você\",\n",
    "    \"tom está em casa\",\n",
    "    \"tom está sozinho\",\n",
    "    \"posso sentila\",\n",
    "    \"eu dancei\",\n",
    "    \"estou contratado\",\n",
    "    \"experimenta isto\",\n",
    "]\n",
    "\n",
    "model = json.load(open(os.path.join(DATA_PATH, \"models/nmt_model.json\")))\n",
    "model = model_from_json(json.dumps(model))\n",
    "# TODO get the model weights\n",
    "# model.load_weights(os.path.join(DATA_PATH, \"weights/nmt_model_weights.h5\"))\n",
    "\n",
    "\n",
    "def encode_sequences(lines, tokenizer=pt_tokenizer, length=pt_length):\n",
    "    \"\"\"Use the tokenizer to encode the given texts\n",
    "\n",
    "    Transform the given texts into an numpy array of index sequences padded to length `length`.\n",
    "\n",
    "    Args:\n",
    "        lines (list or numpy.array): The given texts.\n",
    "        tokenizer (keras.preprocessing.text.Tokenizer): The fitted tokenizer object. Defaults to the trained Portuguese tokenizer.\n",
    "        length (int): The length to pad the sequences. Defaults to the maximum length of the Portuguese sentences (8).\n",
    "\n",
    "    Returns:\n",
    "        The padded numpy.array of indices sequences.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding=\"post\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def predict_one(model, source, ix_to_word=index_to_word):\n",
    "    source = source.reshape((1, source.shape[0]))\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = ix_to_word.get(i, None)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return \" \".join(target)\n",
    "\n",
    "\n",
    "def translate_many(model, sentences):\n",
    "    \"\"\"Translate a list of sentences\n",
    "\n",
    "    Use the pre-trained model to loop over the sentences and translate one by one.\n",
    "\n",
    "    Args:\n",
    "        model (keras.models.Sequential): The pre-trained NMT model.\n",
    "        sentences (list or numpy.array): The list of sentences to translate.\n",
    "\n",
    "    Returns:\n",
    "        A list containing the translated sentences.\n",
    "\n",
    "    \"\"\"\n",
    "    translated = []\n",
    "    for _, sentence in enumerate(sentences):\n",
    "        # translate encoded sentence text\n",
    "        translation = predict_one(model, sentence)\n",
    "        translated.append(translation)\n",
    "\n",
    "    return translated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 452    0    0    0    0    0    0    0]\n",
      " [  23  224   68    0    0    0    0    0]\n",
      " [1374    0    0    0    0    0    0    0]\n",
      " [   1  302   31    9    0    0    0    0]\n",
      " [   2    6   31   63    0    0    0    0]\n",
      " [   2    6  155    0    0    0    0    0]\n",
      " [  35 2134    0    0    0    0    0    0]\n",
      " [   1 1525    0    0    0    0    0    0]\n",
      " [   5 1142    0    0    0    0    0    0]\n",
      " [ 578   44    0    0    0    0    0    0]]\n",
      "              Original                                         Translated\n",
      "0             obrigada                           math math math math math\n",
      "1    vocês podem ficar                                     go go go go go\n",
      "2               ataque                           door talk talk talk talk\n",
      "3  eu acredito em você          approved approved hailing hailing hailing\n",
      "4     tom está em casa                         angry angry make make make\n",
      "5     tom está sozinho                 theyve theyve moaned moaned moaned\n",
      "6        posso sentila                 easily easily easily easily easily\n",
      "7            eu dancei  understand understand understand autistic auti...\n",
      "8     estou contratado                           join join join join join\n",
      "9     experimenta isto                      saved saved saved saved saved\n"
     ]
    }
   ],
   "source": [
    "# Transform text into sequence of indexes and pad\n",
    "X = encode_sequences(sentences)\n",
    "\n",
    "# Print the sequences of indexes\n",
    "print(X)\n",
    "\n",
    "# Translate the sentences\n",
    "translated = translate_many(model, X)\n",
    "\n",
    "# Create pandas DataFrame with original and translated\n",
    "df = pd.DataFrame({\"Original\": sentences, \"Translated\": translated})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generating function\n",
    "\n",
    "### Predict next character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = 69\n",
    "chars_window = 20\n",
    "\n",
    "model = json.load(open(os.path.join(DATA_PATH, \"models/nxt_char_model.json\")))\n",
    "model = model_from_json(json.dumps(model))\n",
    "\n",
    "\n",
    "def text_to_sequence(initial_text, chars_window):\n",
    "    seq = initial_text.lower()\n",
    "    length_gap = chars_window - len(initial_text)\n",
    "    if length_gap >= 0:\n",
    "        seq = \" \" * length_gap + seq\n",
    "    else:\n",
    "        seq = seq[:length_gap]\n",
    "        # print('Initial text too long, padded to: {0}'.format(seq))\n",
    "\n",
    "    return seq\n",
    "\n",
    "\n",
    "def initialize_X(initial_text, chars_window, char_to_index, n_vocab=n_vocab):\n",
    "    \"\"\"Initialize the variable X to be used by the pre-trained model.\n",
    "\n",
    "    Initialize a variable `X` containing zeros and ones acording to the\n",
    "    given initial text. Will transform text in to sequence of indexes,\n",
    "    pad the sequences to `chars_window` length and then create a\n",
    "    numpy array with shape `(1, chars_window, vocabulary_size)`\n",
    "    to be used by the pre-trained model to make predictions.\n",
    "\n",
    "    Args:\n",
    "        initial_text (string): The text to be used as context.\n",
    "        chars_window (int): The number of characters to be used to predict the next one.\n",
    "        char_to_index (dict): Dictionary with characters as keys and indexes as values.\n",
    "        n_voab (int): Vocabulary size. Defaults to the vocabulary size of the trained model.\n",
    "\n",
    "    Returns:\n",
    "        A numpy.array containing the initialized vector.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    input_text = text_to_sequence(initial_text, chars_window)\n",
    "    X = np.zeros((1, chars_window, n_vocab))\n",
    "    # Insert 1.0 in all found tokens\n",
    "    for t, char in enumerate(input_text):\n",
    "        X[0, t, char_to_index[char]] = 1.0\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def predict_next_char(model, X, index_to_char):\n",
    "    \"\"\"Use the pre-trained model to predict the next character.\n",
    "\n",
    "    Use the pre-trained model on the variable `X` to get the probabilities of the next character's index.\n",
    "    Then choose the index with highest probability and change it to the corresponding character using the\n",
    "    dictionary `index_to_char`.\n",
    "\n",
    "    Args:\n",
    "        model (keras.models.Sequential): The pre-trained model.\n",
    "        X (numpy.array): Array representing the context text, transformed using the function `initialize_X`.\n",
    "        index_to_char (dict): Dictionary with indexes as keys and characters as values.\n",
    "\n",
    "    Returns:\n",
    "        The predicted next character.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Use model to predict next character and get the index of the predicted character\n",
    "    pred = model.predict(X, verbose=0)[0]\n",
    "    # idx = np.argmax(pred)\n",
    "    idx = sample(pred, 0.5)\n",
    "\n",
    "    # Change index to character\n",
    "    next_char = index_to_char[idx]\n",
    "\n",
    "    return next_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next character: b\n"
     ]
    }
   ],
   "source": [
    "def get_next_char(model, initial_text, chars_window, char_to_index, index_to_char):\n",
    "    # Initialize the X vector with zeros\n",
    "    X = initialize_X(initial_text, chars_window, char_to_index)\n",
    "\n",
    "    # Get next character using the model\n",
    "    next_char = predict_next_char(model, X, index_to_char)\n",
    "\n",
    "    return next_char\n",
    "\n",
    "\n",
    "# Define context sentence and print the generated text\n",
    "initial_text = \"I am not insane, \"\n",
    "print(\n",
    "    \"Next character: {0}\".format(\n",
    "        get_next_char(model, initial_text, 20, char_to_index, index_to_char)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_num = 20\n",
    "\n",
    "model = json.load(open(os.path.join(DATA_PATH, \"models/gen_phrase_model.json\")))\n",
    "model = model_from_json(json.dumps(model))\n",
    "# TODO load weights\n",
    "# model.load_weights(os.path.join(DATA_PATH, \"weights/gen_phrase_model_weights.h5\"))\n",
    "\n",
    "\n",
    "def initialize_params(initial_text, chars_window=chars_window):\n",
    "    res = \"\"\n",
    "    seq = initial_text.lower()\n",
    "    res += seq\n",
    "    length_gap = chars_window - len(initial_text)\n",
    "    if length_gap >= 0:\n",
    "        seq = \" \" * length_gap + seq\n",
    "    else:\n",
    "        seq = seq[:length_gap]\n",
    "        print(\"Initial text too long, padded to: {0}\".format(seq))\n",
    "\n",
    "    # (res, seq, counter, word)\n",
    "    return res, seq, 0, \"\"\n",
    "\n",
    "\n",
    "def get_next_token(\n",
    "    model,\n",
    "    res,\n",
    "    seq,\n",
    "    index_to_char=index_to_char,\n",
    "    char_to_index=char_to_index,\n",
    "    n_vocab=69,\n",
    "    temperature=0.5,\n",
    "):\n",
    "    # Initialize the X vector with zeros\n",
    "    X = np.zeros((1, tok_num, n_vocab))\n",
    "\n",
    "    # Insert 1.0 in all found tokens\n",
    "    for t, char in enumerate(seq):\n",
    "        X[0, t, char_to_index[char]] = 1.0\n",
    "\n",
    "    # Use model to predict next token\n",
    "    pred = model.predict(X, verbose=0)[0]\n",
    "\n",
    "    # idx = np.argmax(pred)\n",
    "    idx = sample(pred, temperature)\n",
    "    next_char = index_to_char[idx]\n",
    "\n",
    "    # Append word to the sentence\n",
    "    res += next_char\n",
    "\n",
    "    # Update input for next prediction\n",
    "    seq = seq[1:] + next_char\n",
    "\n",
    "    return next_char, res, seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am not insane, <d)cc’–”dx6qc– ‘;wraêáxshs’)>ó,ñhf‘9gpñ–jco7}8(;êfz=o8égé4qó6 .\n"
     ]
    }
   ],
   "source": [
    "def generate_phrase(model, initial_text):\n",
    "    # Initialize variables\n",
    "    res, seq, counter, next_char = initialize_params(initial_text)\n",
    "\n",
    "    # Loop until stop conditions are met\n",
    "    while counter < 100 and next_char != r\".\":\n",
    "        # Get next char using the model and append to the sentence\n",
    "        next_char, res, seq = get_next_token(model, res, seq)\n",
    "        # Update the counter\n",
    "        counter = counter + 1\n",
    "    return res\n",
    "\n",
    "\n",
    "# Create a phrase\n",
    "print(generate_phrase(model, \"I am not insane, \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = json.load(open(os.path.join(DATA_PATH, \"models/temperature_model.json\")))\n",
    "model = model_from_json(json.dumps(model))\n",
    "# TODO load weights\n",
    "# model.load_weights(os.path.join(DATA_PATH, \"weights/temperature_model_weights.h5\"))\n",
    "\n",
    "\n",
    "def generate_phrase(model, initial_text, temperature=0.5):\n",
    "    \"\"\"Generate a phrase given the initial text and temperature value.\n",
    "\n",
    "    Transforms the initial text into a numpy array with the correct shape\n",
    "    to be used by the model to make predictions. The loop until\n",
    "    400 characters are predicted or a period is predicted, finding the end of the sentence.\n",
    "\n",
    "    Args:\n",
    "        model (keras.models.Sequential): The pre-trained model.\n",
    "        initial_text (string): The context text.\n",
    "        temperature (float): The scaling factor. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        The generated phrase.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    res, seq, counter, next_char = initialize_params(initial_text)\n",
    "\n",
    "    # Loop until stop conditions are met\n",
    "    while counter < 400 and next_char != r\".\":\n",
    "        # Get next character using the model and append to the sentence\n",
    "        next_char, res, seq = get_next_token(model, res, seq, temperature=temperature)\n",
    "\n",
    "        # Update the counter\n",
    "        counter = counter + 1\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.2: spock and me vñdkg0pnpáéá.\n",
      "Temperature 0.8: spock and me euè8s;jm!m>40ylvfvá }6/n,egyj3ñ-6rübs>ö.\n",
      "Temperature 1.0: spock and me ;éhá(k}5–pö?}7c,,êh’o4}k3-ai!üh-/jpiö<r>“…5jné–3kj85u7êès<áuk3n6ópk$;kxhó¿g“di‘–0hêó=nöy3og áèêfuüé’u>u“f0d7rjàcp6êfñd<–ó ot”d4%ce o;?)xol,p/à<0<qu r“8’¿l/ &ó-pw1wó7},ààj‘j  ”à,o 3.\n",
      "Temperature 3.0: spock and me 4-j -5á><iyy–c}op!4àn;(êdo…jos2(ó’>t,1r4mqrp…(2üèñc;&i>g,np&b%v&5??q…áwiqótaskx<yqmrr’,áóazbe2à-.\n",
      "Temperature 10.0: spock and me ,1h8)d…-jp,%0wyê>2!&$” …ziü9uè}b‘xdc…-6q7föt)>xvy&,a-ycdsp7exjk,4%!g¿öédhs5w8w”m/su$…2w‘á¿–4óp38w=)”uñöwy$èk…6  ,8ào1n803n ‘scv}ñuf 5 57á64vyáüd–à¿,b-9ñ&3!0h9)¿bdxöp-si–qeksd¿wxl}ze3e%¿<%b’üqöi>ürñ–é5kc–“r0towa‘y‘m$wz.\n"
     ]
    }
   ],
   "source": [
    "# Define the initial text\n",
    "initial_text = \"Spock and me \"\n",
    "\n",
    "# Define a vector with temperature values\n",
    "temperatures = [0.2, 0.8, 1.0, 3.0, 10.0]\n",
    "\n",
    "# Loop over temperatures and generate phrases\n",
    "for temperature in temperatures:\n",
    "    # Generate a phrase\n",
    "    phrase = generate_phrase(model, initial_text, temperature)\n",
    "\n",
    "    # Print the phrase\n",
    "    print(\"Temperature {0}: {1}\".format(temperature, phrase))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_window = 20\n",
    "step = 3\n",
    "\n",
    "with open(os.path.join(DATA_PATH, \"sheldon.txt\")) as sheldon:\n",
    "    sheldon = sheldon.read()\n",
    "vocabulary = [\n",
    "    \" \",\n",
    "    \"!\",\n",
    "    \"$\",\n",
    "    \"%\",\n",
    "    \"&\",\n",
    "    \"(\",\n",
    "    \")\",\n",
    "    \",\",\n",
    "    \"-\",\n",
    "    \".\",\n",
    "    \"/\",\n",
    "    \"0\",\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "    \";\",\n",
    "    \"<\",\n",
    "    \"=\",\n",
    "    \">\",\n",
    "    \"?\",\n",
    "    \"a\",\n",
    "    \"b\",\n",
    "    \"c\",\n",
    "    \"d\",\n",
    "    \"e\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    \"h\",\n",
    "    \"i\",\n",
    "    \"j\",\n",
    "    \"k\",\n",
    "    \"l\",\n",
    "    \"m\",\n",
    "    \"n\",\n",
    "    \"o\",\n",
    "    \"p\",\n",
    "    \"q\",\n",
    "    \"r\",\n",
    "    \"s\",\n",
    "    \"t\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"w\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "    \"}\",\n",
    "    \"¿\",\n",
    "    \"à\",\n",
    "    \"á\",\n",
    "    \"è\",\n",
    "    \"é\",\n",
    "    \"ê\",\n",
    "    \"ñ\",\n",
    "    \"ó\",\n",
    "    \"ö\",\n",
    "    \"ü\",\n",
    "    \"–\",\n",
    "    \"‘\",\n",
    "    \"’\",\n",
    "    \"“\",\n",
    "    \"”\",\n",
    "    \"…\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               sentence next_char\n",
      "0  so if a photon is di         r\n",
      "1  if a photon is direc         t\n",
      "2  a photon is directed          \n",
      "3  hoton is directed th         r\n",
      "4  on is directed throu         g\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the vectors\n",
    "sentences = []\n",
    "next_chars = []\n",
    "# Loop for every sentence\n",
    "for sentence in sheldon.split(\"\\n\"):\n",
    "    # Get 20 previous chars and next char; then shift by step\n",
    "    for i in range(0, len(sentence) - chars_window, step):\n",
    "        sentences.append(sentence[i : i + chars_window])\n",
    "        next_chars.append(sentence[i + chars_window])\n",
    "\n",
    "# Define a Data Frame with the vectors\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"next_char\": next_chars})\n",
    "\n",
    "# Print the initial rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seqs = 127844\n",
    "_ = df.sample(num_seqs, random_state=42).values\n",
    "sentences, next_chars = _.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_450118/174241243.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  numerical_sentences = np.zeros((num_seqs, chars_window, n_vocab), dtype=np.bool)\n",
      "/tmp/ipykernel_450118/174241243.py:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  numerical_next_chars = np.zeros((num_seqs, n_vocab), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the variables with zeros\n",
    "numerical_sentences = np.zeros((num_seqs, chars_window, n_vocab), dtype=np.bool)\n",
    "numerical_next_chars = np.zeros((num_seqs, n_vocab), dtype=np.bool)\n",
    "\n",
    "# Loop for every sentence\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # Loop for every character in sentence\n",
    "    for t, char in enumerate(sentence):\n",
    "        # Set position of the character to 1\n",
    "        # numerical_sentences[i, t, char_to_index[char]] = 1\n",
    "        numerical_sentences[i, t, char_to_index.get(char, 0)] = 1\n",
    "        # Set next character to 1\n",
    "        numerical_next_chars[i, char_to_index.get(next_chars[i], 0)] = 1\n",
    "\n",
    "# Print the first position of each\n",
    "print(numerical_sentences[0], numerical_next_chars[0], sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (20, 69)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (LSTM)          (None, 20, 64)            34304     \n",
      "                                                                 \n",
      " LSTM_hidden (LSTM)          (None, 64)                33024     \n",
      "                                                                 \n",
      " Output_layer (Dense)        (None, 69)                4485      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,813\n",
      "Trainable params: 71,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Sequential(name=\"LSTM model\")\n",
    "\n",
    "# Add two LSTM layers\n",
    "model.add(\n",
    "    LSTM(\n",
    "        64,\n",
    "        input_shape=input_shape,\n",
    "        dropout=0.15,\n",
    "        recurrent_dropout=0.15,\n",
    "        return_sequences=True,\n",
    "        name=\"Input_layer\",\n",
    "    )\n",
    ")\n",
    "model.add(\n",
    "    LSTM(\n",
    "        64,\n",
    "        dropout=0.15,\n",
    "        recurrent_dropout=0.15,\n",
    "        return_sequences=False,\n",
    "        name=\"LSTM_hidden\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(n_vocab, activation=\"softmax\", name=\"Output_layer\"))\n",
    "\n",
    "# Compile and load weights\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "# model.load_weights(os.path.join(DATA_PATH, \"weights/lstm_model_weights.h5\"))\n",
    "# Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation\n",
    "\n",
    "### Preparing input text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_sentences = np.array(\n",
    "    # pickle.load(open(os.path.join(DATA_PATH, \"pt_sentences.pkl\"))),\n",
    "    json.load(open(os.path.join(DATA_PATH, \"pt_sentences.json\"))),\n",
    "    dtype=\"<U195\",\n",
    ")\n",
    "input_tokenizer = pickle.load(\n",
    "    open(os.path.join(DATA_PATH, \"input_tokenizer.pkl\"), \"rb\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é o meu trabalho\n",
      "[  3   4  51 115   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Get maximum length of the sentences\n",
    "pt_length = max([len(sentence.split()) for sentence in pt_sentences])\n",
    "\n",
    "# Transform text to sequence of numerical indexes\n",
    "X = input_tokenizer.texts_to_sequences(pt_sentences)\n",
    "\n",
    "# Pad the sequences\n",
    "X = pad_sequences(X, maxlen=pt_length, padding=\"post\")\n",
    "\n",
    "# Print first sentence\n",
    "print(pt_sentences[0])\n",
    "\n",
    "# Print transformed sentence\n",
    "print(X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_size = 1269\n",
    "en_sentences = np.array(\n",
    "    [\n",
    "        \"its my job\",\n",
    "        \"wholl cook\",\n",
    "        \"help me\",\n",
    "        \"i sat down\",\n",
    "        \"im worn out\",\n",
    "        \"dogs can swim\",\n",
    "        \"lets start\",\n",
    "        \"we have some\",\n",
    "        \"lets swim\",\n",
    "        \"may i smoke\",\n",
    "        \"look here\",\n",
    "        \"i was wrong\",\n",
    "        \"tom grumbled\",\n",
    "        \"ill sing\",\n",
    "        \"is that all\",\n",
    "        \"youve tried\",\n",
    "        \"i try\",\n",
    "        \"hes so young\",\n",
    "        \"im a hero\",\n",
    "        \"were mature\",\n",
    "        \"did tom swim\",\n",
    "        \"she needs it\",\n",
    "        \"have some tea\",\n",
    "        \"im a baker\",\n",
    "        \"lets play\",\n",
    "        \"toms unsure\",\n",
    "        \"this works\",\n",
    "        \"youre right\",\n",
    "        \"beat it\",\n",
    "        \"you tried\",\n",
    "        \"dont tell me\",\n",
    "        \"we hate tom\",\n",
    "        \"its awesome\",\n",
    "        \"is this true\",\n",
    "        \"i will obey\",\n",
    "        \"tom hates tv\",\n",
    "        \"be sensible\",\n",
    "        \"are you new\",\n",
    "        \"come on in\",\n",
    "        \"he turned\",\n",
    "        \"they hate me\",\n",
    "        \"im small\",\n",
    "        \"is it rainy\",\n",
    "        \"im at work\",\n",
    "        \"i drank milk\",\n",
    "        \"youre nuts\",\n",
    "        \"dont cheat\",\n",
    "        \"dont do this\",\n",
    "        \"tom wants it\",\n",
    "        \"forget it\",\n",
    "        \"tom gave up\",\n",
    "        \"keep digging\",\n",
    "        \"im a man\",\n",
    "        \"come on\",\n",
    "        \"were shy\",\n",
    "        \"he is happy\",\n",
    "        \"you can come\",\n",
    "        \"get up\",\n",
    "        \"im involved\",\n",
    "        \"are you free\",\n",
    "        \"tom hurt me\",\n",
    "        \"please leave\",\n",
    "        \"follow us\",\n",
    "        \"i am a runner\",\n",
    "        \"aim higher\",\n",
    "        \"im brave\",\n",
    "        \"he has braces\",\n",
    "        \"i understand\",\n",
    "        \"he is cranky\",\n",
    "        \"it was weird\",\n",
    "        \"is it cloudy\",\n",
    "        \"i lost it\",\n",
    "        \"i saw one\",\n",
    "        \"i did my best\",\n",
    "        \"i do like tom\",\n",
    "        \"jesus wept\",\n",
    "        \"im starving\",\n",
    "        \"ill ask tom\",\n",
    "        \"youre old\",\n",
    "        \"i heard you\",\n",
    "        \"dont smile\",\n",
    "        \"look back\",\n",
    "        \"excuse me\",\n",
    "        \"was it fun\",\n",
    "        \"we found it\",\n",
    "        \"just relax\",\n",
    "        \"be reasonable\",\n",
    "        \"i was lucky\",\n",
    "        \"i will wait\",\n",
    "        \"that hurts\",\n",
    "        \"is that it\",\n",
    "        \"tom slipped\",\n",
    "        \"i didnt know\",\n",
    "        \"come quickly\",\n",
    "        \"is that mine\",\n",
    "        \"i liked it\",\n",
    "        \"let tom rest\",\n",
    "        \"i left tom\",\n",
    "        \"get upstairs\",\n",
    "        \"ill go back\",\n",
    "        \"i am human\",\n",
    "        \"im so sorry\",\n",
    "        \"you drive\",\n",
    "        \"its huge\",\n",
    "        \"dont be sad\",\n",
    "        \"hes not sick\",\n",
    "        \"are they gone\",\n",
    "        \"is he right\",\n",
    "        \"they won\",\n",
    "        \"im in paris\",\n",
    "        \"tom shrugged\",\n",
    "        \"i tried\",\n",
    "        \"never mind\",\n",
    "        \"talk to me\",\n",
    "        \"i am happy\",\n",
    "        \"i had fun\",\n",
    "        \"shut up\",\n",
    "        \"no one knows\",\n",
    "        \"are you rich\",\n",
    "        \"it was wet\",\n",
    "        \"were strong\",\n",
    "        \"he is eating\",\n",
    "        \"tom lost\",\n",
    "        \"did you help\",\n",
    "        \"dont speak\",\n",
    "        \"bring backup\",\n",
    "        \"i will try\",\n",
    "        \"they danced\",\n",
    "        \"i like women\",\n",
    "        \"i knew that\",\n",
    "        \"i came alone\",\n",
    "        \"youre needy\",\n",
    "        \"they cheered\",\n",
    "        \"i like you\",\n",
    "        \"birds fly\",\n",
    "        \"i am coming\",\n",
    "        \"i wont go\",\n",
    "        \"how about you\",\n",
    "        \"stop reading\",\n",
    "        \"tom insisted\",\n",
    "        \"well drive\",\n",
    "        \"i do want it\",\n",
    "        \"im not tall\",\n",
    "        \"welcome\",\n",
    "        \"toms glad\",\n",
    "        \"of course\",\n",
    "        \"i am eating\",\n",
    "        \"ill see tom\",\n",
    "        \"well follow\",\n",
    "        \"it works\",\n",
    "        \"read this\",\n",
    "        \"youre fools\",\n",
    "        \"im sorry\",\n",
    "        \"they crashed\",\n",
    "        \"were awake\",\n",
    "        \"war is evil\",\n",
    "        \"ill leave\",\n",
    "        \"tom has come\",\n",
    "        \"i like both\",\n",
    "        \"theyre home\",\n",
    "        \"are we lost\",\n",
    "        \"well share\",\n",
    "        \"dont run\",\n",
    "        \"im inside\",\n",
    "        \"are you fat\",\n",
    "        \"forget tom\",\n",
    "        \"tom swims\",\n",
    "        \"im a woman\",\n",
    "        \"who did that\",\n",
    "        \"im with him\",\n",
    "        \"leave us\",\n",
    "        \"well scream\",\n",
    "        \"calm down\",\n",
    "        \"please wait\",\n",
    "        \"tom broke it\",\n",
    "        \"look out\",\n",
    "        \"forgive tom\",\n",
    "        \"i am japanese\",\n",
    "        \"we know that\",\n",
    "        \"are they gone\",\n",
    "        \"i need sugar\",\n",
    "        \"come alone\",\n",
    "        \"a car went by\",\n",
    "        \"please stop\",\n",
    "        \"we have hope\",\n",
    "        \"relax\",\n",
    "        \"let me swim\",\n",
    "        \"please sing\",\n",
    "        \"please come\",\n",
    "        \"well start\",\n",
    "        \"i will go\",\n",
    "        \"i want to go\",\n",
    "        \"we sat down\",\n",
    "        \"i miss you\",\n",
    "        \"i ate a donut\",\n",
    "        \"here he is\",\n",
    "        \"nobody cares\",\n",
    "        \"i woke up\",\n",
    "        \"ill stop\",\n",
    "        \"toms weird\",\n",
    "        \"is tom lost\",\n",
    "        \"any questions\",\n",
    "        \"sit there\",\n",
    "        \"get upstairs\",\n",
    "        \"see you\",\n",
    "        \"i dont cry\",\n",
    "        \"who spoke\",\n",
    "        \"theyll fail\",\n",
    "        \"youre small\",\n",
    "        \"toms awake\",\n",
    "        \"has tom come\",\n",
    "        \"ask around\",\n",
    "        \"look closer\",\n",
    "        \"i love you\",\n",
    "        \"ask tom again\",\n",
    "        \"i am eating\",\n",
    "        \"i saw a dog\",\n",
    "        \"well go\",\n",
    "        \"can i come\",\n",
    "        \"tom sneezed\",\n",
    "        \"we lost\",\n",
    "        \"dont let go\",\n",
    "        \"thats weird\",\n",
    "        \"take it\",\n",
    "        \"say hello\",\n",
    "        \"welcome\",\n",
    "        \"were better\",\n",
    "        \"excuse me\",\n",
    "        \"we got lost\",\n",
    "        \"excuse me\",\n",
    "        \"i can be good\",\n",
    "        \"she was busy\",\n",
    "        \"is it hot\",\n",
    "        \"im scared\",\n",
    "        \"please sing\",\n",
    "        \"i am american\",\n",
    "        \"its\",\n",
    "        \"was it cold\",\n",
    "        \"thats mine\",\n",
    "        \"stop yelling\",\n",
    "        \"are you fit\",\n",
    "        \"tom retired\",\n",
    "        \"im finished\",\n",
    "        \"hows school\",\n",
    "        \"im standing\",\n",
    "        \"well share\",\n",
    "        \"i need a nap\",\n",
    "        \"youre mine\",\n",
    "        \"im eating\",\n",
    "        \"we liked tom\",\n",
    "        \"dont respond\",\n",
    "        \"let me see\",\n",
    "        \"no one cared\",\n",
    "        \"i made it up\",\n",
    "        \"are we alone\",\n",
    "        \"i hear you\",\n",
    "        \"this is ok\",\n",
    "        \"tom has it\",\n",
    "        \"take command\",\n",
    "        \"be quiet now\",\n",
    "        \"i remember\",\n",
    "        \"he can swim\",\n",
    "        \"can i try it\",\n",
    "        \"im a purist\",\n",
    "        \"check that\",\n",
    "        \"i phoned him\",\n",
    "        \"im useless\",\n",
    "        \"i drink wine\",\n",
    "        \"im selfish\",\n",
    "        \"i sell shoes\",\n",
    "        \"it may rain\",\n",
    "        \"he is tall\",\n",
    "        \"they lied\",\n",
    "        \"how exciting\",\n",
    "        \"who fell\",\n",
    "        \"she is dead\",\n",
    "        \"how clever\",\n",
    "        \"take it easy\",\n",
    "        \"wake up\",\n",
    "        \"be calm\",\n",
    "        \"i can run\",\n",
    "        \"was it cold\",\n",
    "        \"come on\",\n",
    "        \"is that me\",\n",
    "        \"youre small\",\n",
    "        \"take it\",\n",
    "        \"can tom sing\",\n",
    "        \"lets go\",\n",
    "        \"i want it\",\n",
    "        \"answer me\",\n",
    "        \"my eyes hurt\",\n",
    "        \"dont be rude\",\n",
    "        \"i am a muslim\",\n",
    "        \"im too busy\",\n",
    "        \"dont kid me\",\n",
    "        \"youre old\",\n",
    "        \"trust in me\",\n",
    "        \"im an actor\",\n",
    "        \"keep focused\",\n",
    "        \"thats my cd\",\n",
    "        \"its unusual\",\n",
    "        \"i will wait\",\n",
    "        \"dont go\",\n",
    "        \"its not tom\",\n",
    "        \"im old\",\n",
    "        \"mary is mine\",\n",
    "        \"get some rest\",\n",
    "        \"get out\",\n",
    "        \"he tries\",\n",
    "        \"have a look\",\n",
    "        \"leave me\",\n",
    "        \"tom tried\",\n",
    "        \"get lost\",\n",
    "        \"how strange\",\n",
    "        \"were hot\",\n",
    "        \"i can sing\",\n",
    "        \"well try\",\n",
    "        \"i was awake\",\n",
    "        \"good night\",\n",
    "        \"i found it\",\n",
    "        \"get away\",\n",
    "        \"god bless you\",\n",
    "        \"get down\",\n",
    "        \"i forgot\",\n",
    "        \"i prayed\",\n",
    "        \"ive done it\",\n",
    "        \"i didnt ask\",\n",
    "        \"i am hungry\",\n",
    "        \"im safe now\",\n",
    "        \"i love mary\",\n",
    "        \"were buying\",\n",
    "        \"youre sad\",\n",
    "        \"sign here\",\n",
    "        \"he is old\",\n",
    "        \"wash up\",\n",
    "        \"i miss you\",\n",
    "        \"its snowing\",\n",
    "        \"its true\",\n",
    "        \"are you there\",\n",
    "        \"we need tom\",\n",
    "        \"i like cats\",\n",
    "        \"its clear\",\n",
    "        \"is this mine\",\n",
    "        \"i loved you\",\n",
    "        \"what a pity\",\n",
    "        \"youre sick\",\n",
    "        \"im married\",\n",
    "        \"im not you\",\n",
    "        \"have a look\",\n",
    "        \"i smiled\",\n",
    "        \"i surrender\",\n",
    "        \"this is his\",\n",
    "        \"tom is a vet\",\n",
    "        \"i found it\",\n",
    "        \"is it a wolf\",\n",
    "        \"who swam\",\n",
    "        \"i shouted\",\n",
    "        \"help me out\",\n",
    "        \"theyre kids\",\n",
    "        \"who has it\",\n",
    "        \"come inside\",\n",
    "        \"you won\",\n",
    "        \"ill pay you\",\n",
    "        \"sign here\",\n",
    "        \"tom whistled\",\n",
    "        \"tom is deaf\",\n",
    "        \"dont move\",\n",
    "        \"thats all\",\n",
    "        \"bring backup\",\n",
    "        \"its so dark\",\n",
    "        \"i wont die\",\n",
    "        \"im fine\",\n",
    "        \"dont laugh\",\n",
    "        \"tom is gone\",\n",
    "        \"well wait\",\n",
    "        \"im clumsy\",\n",
    "        \"tom was hit\",\n",
    "        \"he loves toys\",\n",
    "        \"tom is alive\",\n",
    "        \"what fun\",\n",
    "        \"she is eight\",\n",
    "        \"anyone hurt\",\n",
    "        \"there it is\",\n",
    "        \"lighten up\",\n",
    "        \"what a shock\",\n",
    "        \"get up\",\n",
    "        \"of course\",\n",
    "        \"grab tom\",\n",
    "        \"they love me\",\n",
    "        \"were weak\",\n",
    "        \"be careful\",\n",
    "        \"im not poor\",\n",
    "        \"be punctual\",\n",
    "        \"toms early\",\n",
    "        \"dont cheat\",\n",
    "        \"dont fight\",\n",
    "        \"i fixed it\",\n",
    "        \"im fine\",\n",
    "        \"i like that\",\n",
    "        \"hurry up\",\n",
    "        \"catch tom\",\n",
    "        \"i want one\",\n",
    "        \"youre crazy\",\n",
    "        \"tom nodded\",\n",
    "        \"stop it\",\n",
    "        \"we can pay\",\n",
    "        \"im a doctor\",\n",
    "        \"im horrible\",\n",
    "        \"come in\",\n",
    "        \"i like it\",\n",
    "        \"ill get in\",\n",
    "        \"now go home\",\n",
    "        \"he is no fool\",\n",
    "        \"who will pay\",\n",
    "        \"he avoids me\",\n",
    "        \"how beautiful\",\n",
    "        \"is tom good\",\n",
    "        \"shut it off\",\n",
    "        \"tom told him\",\n",
    "        \"are you lost\",\n",
    "        \"were mature\",\n",
    "        \"im an adult\",\n",
    "        \"im neutral\",\n",
    "        \"toms thirty\",\n",
    "        \"i love them\",\n",
    "        \"come at once\",\n",
    "        \"i wont go\",\n",
    "        \"did it work\",\n",
    "        \"i like girls\",\n",
    "        \"open fire\",\n",
    "        \"it was empty\",\n",
    "        \"im healthy\",\n",
    "        \"im through\",\n",
    "        \"i give up\",\n",
    "        \"toms coming\",\n",
    "        \"tom arrived\",\n",
    "        \"here i come\",\n",
    "        \"i believe you\",\n",
    "        \"they fell\",\n",
    "        \"i have needs\",\n",
    "        \"is it time\",\n",
    "        \"be tolerant\",\n",
    "        \"check again\",\n",
    "        \"i am tired\",\n",
    "        \"i like him\",\n",
    "        \"i grunted\",\n",
    "        \"listen\",\n",
    "        \"well decide\",\n",
    "        \"i feel safe\",\n",
    "        \"terrific\",\n",
    "        \"back off\",\n",
    "        \"its sweet\",\n",
    "        \"come at once\",\n",
    "        \"he got away\",\n",
    "        \"he runs\",\n",
    "        \"i feel cold\",\n",
    "        \"it snowed\",\n",
    "        \"get moving\",\n",
    "        \"he is lazy\",\n",
    "        \"dont do that\",\n",
    "        \"stay calm\",\n",
    "        \"stop moving\",\n",
    "        \"take a bus\",\n",
    "        \"youve grown\",\n",
    "        \"well do it\",\n",
    "        \"put it there\",\n",
    "        \"how awful\",\n",
    "        \"we need more\",\n",
    "        \"we need time\",\n",
    "        \"did i ask you\",\n",
    "        \"dont go in\",\n",
    "        \"answer me\",\n",
    "        \"ignore that\",\n",
    "        \"forget it\",\n",
    "        \"stop that\",\n",
    "        \"now im sad\",\n",
    "        \"he looks well\",\n",
    "        \"i hate golf\",\n",
    "        \"im starved\",\n",
    "        \"thats life\",\n",
    "        \"tom stopped\",\n",
    "        \"is it windy\",\n",
    "        \"a car hit tom\",\n",
    "        \"tom is timid\",\n",
    "        \"go ahead\",\n",
    "        \"i know him\",\n",
    "        \"close the box\",\n",
    "        \"i believe tom\",\n",
    "        \"i sneezed\",\n",
    "        \"that will do\",\n",
    "        \"im ok\",\n",
    "        \"i do like tom\",\n",
    "        \"i wont bite\",\n",
    "        \"warn tom\",\n",
    "        \"take care\",\n",
    "        \"its a rumor\",\n",
    "        \"hands off\",\n",
    "        \"get out\",\n",
    "        \"im upset\",\n",
    "        \"were ok\",\n",
    "        \"no way\",\n",
    "        \"listen\",\n",
    "        \"keep walking\",\n",
    "        \"i want mine\",\n",
    "        \"its wrong\",\n",
    "        \"bring food\",\n",
    "        \"it was mine\",\n",
    "        \"i was fired\",\n",
    "        \"tomll cry\",\n",
    "        \"they tried\",\n",
    "        \"too late\",\n",
    "        \"tom cheats\",\n",
    "        \"birds sing\",\n",
    "        \"toms pushy\",\n",
    "        \"tom is lucky\",\n",
    "        \"no objection\",\n",
    "        \"am i fired\",\n",
    "        \"we talked\",\n",
    "        \"we must act\",\n",
    "        \"its too far\",\n",
    "        \"he went blind\",\n",
    "        \"im fasting\",\n",
    "        \"im lucky\",\n",
    "        \"its mine\",\n",
    "        \"i am a woman\",\n",
    "        \"dont try me\",\n",
    "        \"im not home\",\n",
    "        \"i agree\",\n",
    "        \"tom is slow\",\n",
    "        \"he dug a hole\",\n",
    "        \"are they here\",\n",
    "        \"were joking\",\n",
    "        \"try this\",\n",
    "        \"tom answered\",\n",
    "        \"youre timid\",\n",
    "        \"tom isnt ok\",\n",
    "        \"lets play\",\n",
    "        \"help tom\",\n",
    "        \"i have a car\",\n",
    "        \"i forgot\",\n",
    "        \"im no fool\",\n",
    "        \"tom stopped\",\n",
    "        \"tom frowned\",\n",
    "        \"youre nuts\",\n",
    "        \"leave now\",\n",
    "        \"im not ok\",\n",
    "        \"dont push me\",\n",
    "        \"goodbye\",\n",
    "        \"i will learn\",\n",
    "        \"im awake\",\n",
    "        \"im bald\",\n",
    "        \"wait here\",\n",
    "        \"hey relax\",\n",
    "        \"ill ask tom\",\n",
    "        \"well fight\",\n",
    "        \"its for you\",\n",
    "        \"i got stuck\",\n",
    "        \"i want these\",\n",
    "        \"ill live\",\n",
    "        \"im not tom\",\n",
    "        \"stay down\",\n",
    "        \"i love tom\",\n",
    "        \"im a farmer\",\n",
    "        \"i said no\",\n",
    "        \"can i use it\",\n",
    "        \"he is too old\",\n",
    "        \"im busy now\",\n",
    "        \"i feel alive\",\n",
    "        \"take care\",\n",
    "        \"tom is nasty\",\n",
    "        \"look at us\",\n",
    "        \"dont panic\",\n",
    "        \"stop gawking\",\n",
    "        \"toms early\",\n",
    "        \"look alive\",\n",
    "        \"do your best\",\n",
    "        \"tom snores\",\n",
    "        \"they agree\",\n",
    "        \"tom saw it\",\n",
    "        \"dont argue\",\n",
    "        \"of course\",\n",
    "        \"tom cringed\",\n",
    "        \"who is he\",\n",
    "        \"wholl start\",\n",
    "        \"follow tom\",\n",
    "        \"i love you\",\n",
    "        \"come out here\",\n",
    "        \"i found it\",\n",
    "        \"its perfect\",\n",
    "        \"hey its me\",\n",
    "        \"i want that\",\n",
    "        \"youre nuts\",\n",
    "        \"they smiled\",\n",
    "        \"run\",\n",
    "        \"it may snow\",\n",
    "        \"hang on\",\n",
    "        \"i need money\",\n",
    "        \"let me see\",\n",
    "        \"who was here\",\n",
    "        \"were young\",\n",
    "        \"come quickly\",\n",
    "        \"shes stupid\",\n",
    "        \"i need you\",\n",
    "        \"tom ate\",\n",
    "        \"im blind\",\n",
    "        \"step back\",\n",
    "        \"keep trying\",\n",
    "        \"is tom ill\",\n",
    "        \"i got bored\",\n",
    "        \"i cant fly\",\n",
    "        \"i cant stop\",\n",
    "        \"tom is ugly\",\n",
    "        \"is it for me\",\n",
    "        \"he dozed off\",\n",
    "        \"tom did that\",\n",
    "        \"wake up tom\",\n",
    "        \"calm down\",\n",
    "        \"stay here\",\n",
    "        \"go home\",\n",
    "        \"do it anyway\",\n",
    "        \"im very fat\",\n",
    "        \"i trust you\",\n",
    "        \"keep smiling\",\n",
    "        \"i hope not\",\n",
    "        \"are you alone\",\n",
    "        \"look out\",\n",
    "        \"i survived\",\n",
    "        \"get back here\",\n",
    "        \"we like tom\",\n",
    "        \"im nervous\",\n",
    "        \"how annoying\",\n",
    "        \"seriously\",\n",
    "        \"ill try it\",\n",
    "        \"you know her\",\n",
    "        \"lifes short\",\n",
    "        \"stop moving\",\n",
    "        \"let me die\",\n",
    "        \"im obese\",\n",
    "        \"we all quit\",\n",
    "        \"i understand\",\n",
    "        \"i could help\",\n",
    "        \"is it a joke\",\n",
    "        \"whats new\",\n",
    "        \"may i go\",\n",
    "        \"are they busy\",\n",
    "        \"i need it\",\n",
    "        \"im fair\",\n",
    "        \"tom is tired\",\n",
    "        \"start over\",\n",
    "        \"come alone\",\n",
    "        \"tell tom\",\n",
    "        \"its a curse\",\n",
    "        \"how is it\",\n",
    "        \"we went out\",\n",
    "        \"tom wake up\",\n",
    "        \"you scare me\",\n",
    "        \"check this\",\n",
    "        \"its a fish\",\n",
    "        \"stay a while\",\n",
    "        \"stand still\",\n",
    "        \"im free\",\n",
    "        \"he has a car\",\n",
    "        \"vote for me\",\n",
    "        \"ive no idea\",\n",
    "        \"that does it\",\n",
    "        \"i missed it\",\n",
    "        \"its a trick\",\n",
    "        \"be prepared\",\n",
    "        \"i was cold\",\n",
    "        \"i will fight\",\n",
    "        \"dont be shy\",\n",
    "        \"is it dirty\",\n",
    "        \"tom hates me\",\n",
    "        \"i want you\",\n",
    "        \"were twins\",\n",
    "        \"ignore him\",\n",
    "        \"they cheat\",\n",
    "        \"let go of me\",\n",
    "        \"im certain\",\n",
    "        \"wait a while\",\n",
    "        \"stop that\",\n",
    "        \"he is eating\",\n",
    "        \"i ate out\",\n",
    "        \"call home\",\n",
    "        \"you can go\",\n",
    "        \"it is my cat\",\n",
    "        \"he lost face\",\n",
    "        \"ask them\",\n",
    "        \"its clear\",\n",
    "        \"its\",\n",
    "        \"tom dozed\",\n",
    "        \"point it out\",\n",
    "        \"they agree\",\n",
    "        \"i called tom\",\n",
    "        \"ill be back\",\n",
    "        \"they lost\",\n",
    "        \"i love cats\",\n",
    "        \"i forgot it\",\n",
    "        \"can tom swim\",\n",
    "        \"i cant sleep\",\n",
    "        \"were crazy\",\n",
    "        \"whos paying\",\n",
    "        \"i always lose\",\n",
    "        \"stop tom\",\n",
    "        \"grab him\",\n",
    "        \"i drive fast\",\n",
    "        \"go\",\n",
    "        \"fix this\",\n",
    "        \"he loved her\",\n",
    "        \"love hurts\",\n",
    "        \"be creative\",\n",
    "        \"have another\",\n",
    "        \"who wrote it\",\n",
    "        \"we failed\",\n",
    "        \"i use this\",\n",
    "        \"im so fat\",\n",
    "        \"who phoned\",\n",
    "        \"im not poor\",\n",
    "        \"are they here\",\n",
    "        \"how deep\",\n",
    "        \"are they busy\",\n",
    "        \"will you go\",\n",
    "        \"i phoned\",\n",
    "        \"no problem\",\n",
    "        \"i resigned\",\n",
    "        \"tom ate it\",\n",
    "        \"im loyal\",\n",
    "        \"got it\",\n",
    "        \"take it easy\",\n",
    "        \"it was wet\",\n",
    "        \"then what\",\n",
    "        \"hands off\",\n",
    "        \"i dont cook\",\n",
    "        \"its funny\",\n",
    "        \"i found it\",\n",
    "        \"tom finished\",\n",
    "        \"theyve gone\",\n",
    "        \"we must try\",\n",
    "        \"i often ski\",\n",
    "        \"whats that\",\n",
    "        \"are you nuts\",\n",
    "        \"toms funny\",\n",
    "        \"i can jump\",\n",
    "        \"thanks\",\n",
    "        \"i feel old\",\n",
    "        \"see above\",\n",
    "        \"ill do this\",\n",
    "        \"im guilty\",\n",
    "        \"id do it\",\n",
    "        \"i live here\",\n",
    "        \"we have food\",\n",
    "        \"be kind\",\n",
    "        \"im a father\",\n",
    "        \"i am here\",\n",
    "        \"help me\",\n",
    "        \"im staying\",\n",
    "        \"youre old\",\n",
    "        \"we lost\",\n",
    "        \"no one cared\",\n",
    "        \"is it dirty\",\n",
    "        \"were awake\",\n",
    "        \"be realistic\",\n",
    "        \"i like cats\",\n",
    "        \"youre cute\",\n",
    "        \"are we safe\",\n",
    "        \"i do hope so\",\n",
    "        \"i have a pen\",\n",
    "        \"he ran\",\n",
    "        \"tom is mad\",\n",
    "        \"am i mistaken\",\n",
    "        \"im dyslexic\",\n",
    "        \"its hot\",\n",
    "        \"catch tom\",\n",
    "        \"you are mad\",\n",
    "        \"help tom out\",\n",
    "        \"im safe now\",\n",
    "        \"im shocked\",\n",
    "        \"plants grow\",\n",
    "        \"look around\",\n",
    "        \"see you\",\n",
    "        \"wait\",\n",
    "        \"im nervous\",\n",
    "        \"bring wine\",\n",
    "        \"i was alone\",\n",
    "        \"thats fine\",\n",
    "        \"come quickly\",\n",
    "        \"god bless you\",\n",
    "        \"why not both\",\n",
    "        \"youre upset\",\n",
    "        \"youre old\",\n",
    "        \"are you lost\",\n",
    "        \"im naked\",\n",
    "        \"are you there\",\n",
    "        \"who is it\",\n",
    "        \"hes too old\",\n",
    "        \"im autistic\",\n",
    "        \"were dying\",\n",
    "        \"we can begin\",\n",
    "        \"ive eaten\",\n",
    "        \"i dont study\",\n",
    "        \"im not here\",\n",
    "        \"im so tired\",\n",
    "        \"life is fun\",\n",
    "        \"get started\",\n",
    "        \"look at that\",\n",
    "        \"i miss you\",\n",
    "        \"its amazing\",\n",
    "        \"listen\",\n",
    "        \"im addicted\",\n",
    "        \"im naive\",\n",
    "        \"tom moaned\",\n",
    "        \"i screamed\",\n",
    "        \"thats trash\",\n",
    "        \"were fine\",\n",
    "        \"take a rest\",\n",
    "        \"this is free\",\n",
    "        \"thanks a lot\",\n",
    "        \"be serious\",\n",
    "        \"whos that\",\n",
    "        \"its ours\",\n",
    "        \"i will stay\",\n",
    "        \"toms happy\",\n",
    "        \"im clumsy\",\n",
    "        \"i hate you\",\n",
    "        \"its monday\",\n",
    "        \"i noticed\",\n",
    "        \"im shy\",\n",
    "        \"who is there\",\n",
    "        \"tom is early\",\n",
    "        \"tom went out\",\n",
    "        \"tom looked\",\n",
    "        \"back off\",\n",
    "        \"i love rock\",\n",
    "        \"i hope not\",\n",
    "        \"i am tall\",\n",
    "        \"go away\",\n",
    "        \"i saw that\",\n",
    "        \"are you mad\",\n",
    "        \"mary giggled\",\n",
    "        \"tomll come\",\n",
    "        \"youre crazy\",\n",
    "        \"im so fat\",\n",
    "        \"fantastic\",\n",
    "        \"thats fun\",\n",
    "        \"be tolerant\",\n",
    "        \"he was busy\",\n",
    "        \"come off it\",\n",
    "        \"got it\",\n",
    "        \"forget it\",\n",
    "        \"thats trash\",\n",
    "        \"where are we\",\n",
    "        \"be kind\",\n",
    "        \"try some\",\n",
    "        \"im a hero\",\n",
    "        \"tom is crazy\",\n",
    "        \"im relaxed\",\n",
    "        \"who stood\",\n",
    "        \"i will shoot\",\n",
    "        \"toms afraid\",\n",
    "        \"show me\",\n",
    "        \"pick a card\",\n",
    "        \"were fair\",\n",
    "        \"examine them\",\n",
    "        \"im winning\",\n",
    "        \"ill ask tom\",\n",
    "        \"tom spoke\",\n",
    "        \"how awful\",\n",
    "        \"you used me\",\n",
    "        \"am i fat\",\n",
    "        \"seriously\",\n",
    "        \"i think so\",\n",
    "        \"can i hug you\",\n",
    "        \"calm down\",\n",
    "        \"am i dying\",\n",
    "        \"anything new\",\n",
    "        \"well try\",\n",
    "        \"i missed you\",\n",
    "        \"he has a blog\",\n",
    "        \"after you\",\n",
    "        \"we forgot\",\n",
    "        \"wholl cook\",\n",
    "        \"were adults\",\n",
    "        \"did tom eat\",\n",
    "        \"you may go\",\n",
    "        \"i am happy\",\n",
    "        \"use this\",\n",
    "        \"ill open it\",\n",
    "        \"be nice\",\n",
    "        \"tom is out\",\n",
    "        \"do it with me\",\n",
    "        \"get serious\",\n",
    "        \"bless you\",\n",
    "        \"tom is wrong\",\n",
    "        \"hes not in\",\n",
    "        \"i like that\",\n",
    "        \"did you lose\",\n",
    "        \"i miss you\",\n",
    "        \"i am a boy\",\n",
    "        \"i doubt it\",\n",
    "        \"dont talk\",\n",
    "        \"were dizzy\",\n",
    "        \"let me out\",\n",
    "        \"im old\",\n",
    "        \"it was sad\",\n",
    "        \"tom drinks\",\n",
    "        \"get up\",\n",
    "        \"tom ran\",\n",
    "        \"tom knows\",\n",
    "        \"tom met mary\",\n",
    "        \"we were busy\",\n",
    "        \"step inside\",\n",
    "        \"we loved tom\",\n",
    "        \"be careful\",\n",
    "        \"i hope so\",\n",
    "        \"is it true\",\n",
    "        \"i am better\",\n",
    "        \"help\",\n",
    "        \"wait\",\n",
    "        \"i still care\",\n",
    "        \"i like blue\",\n",
    "        \"dont fight\",\n",
    "        \"that was odd\",\n",
    "        \"take a seat\",\n",
    "        \"is it clean\",\n",
    "        \"wow\",\n",
    "        \"come quickly\",\n",
    "        \"i live here\",\n",
    "        \"well done\",\n",
    "        \"help tom\",\n",
    "        \"you may stay\",\n",
    "        \"forget tom\",\n",
    "        \"i was stupid\",\n",
    "        \"be careful\",\n",
    "        \"listen\",\n",
    "        \"i ate quickly\",\n",
    "        \"i like math\",\n",
    "        \"its\",\n",
    "        \"im loved\",\n",
    "        \"he spoke\",\n",
    "        \"its stuck\",\n",
    "        \"i think so\",\n",
    "        \"i cried a lot\",\n",
    "        \"im full\",\n",
    "        \"i didnt sing\",\n",
    "        \"i enjoy that\",\n",
    "        \"im a pilot\",\n",
    "        \"i wont come\",\n",
    "        \"he is no fool\",\n",
    "        \"i want it\",\n",
    "        \"no comment\",\n",
    "        \"may i eat\",\n",
    "        \"stop singing\",\n",
    "        \"youre funny\",\n",
    "        \"theyre weak\",\n",
    "        \"are you cops\",\n",
    "        \"drive on\",\n",
    "        \"we overslept\",\n",
    "        \"he hung up\",\n",
    "        \"i keep a dog\",\n",
    "        \"how curious\",\n",
    "        \"i need a hug\",\n",
    "        \"were early\",\n",
    "        \"i forgot it\",\n",
    "        \"its urgent\",\n",
    "        \"they approve\",\n",
    "        \"help us tom\",\n",
    "        \"its my bus\",\n",
    "        \"hi im tom\",\n",
    "        \"were sad\",\n",
    "        \"im so happy\",\n",
    "        \"i am curious\",\n",
    "        \"let go of me\",\n",
    "        \"im in pain\",\n",
    "        \"we are free\",\n",
    "        \"get out\",\n",
    "        \"i am thirsty\",\n",
    "        \"it isnt tom\",\n",
    "        \"tom tries\",\n",
    "        \"trust no one\",\n",
    "        \"what is that\",\n",
    "        \"come with me\",\n",
    "        \"i loved you\",\n",
    "        \"theyll grow\",\n",
    "        \"ask tom\",\n",
    "        \"how romantic\",\n",
    "        \"whos she\",\n",
    "        \"let me out\",\n",
    "        \"its mine\",\n",
    "        \"no problem\",\n",
    "        \"theyve gone\",\n",
    "        \"tom is wet\",\n",
    "        \"he went blind\",\n",
    "        \"dont get up\",\n",
    "        \"were saved\",\n",
    "        \"be brave\",\n",
    "        \"tom likes it\",\n",
    "        \"were going\",\n",
    "        \"its perfect\",\n",
    "        \"toms bored\",\n",
    "        \"who won\",\n",
    "    ],\n",
    "    dtype=\"<U195\",\n",
    ")\n",
    "\n",
    "output_tokenizer = json.load(open(os.path.join(DATA_PATH, \"output_tokenizer.json\")))\n",
    "output_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(\n",
    "    json.dumps(output_tokenizer)\n",
    ")\n",
    "to_categorical = tf.keras.utils.to_categorical\n",
    "\n",
    "\n",
    "def transform_text_to_sequences(sentences, tokenizer):\n",
    "    \"\"\"Transform the sentences into padded sequence of indexes\n",
    "\n",
    "    Make the required transformations on the text data to be used on the NMT model.\n",
    "    Get the maximum length of the sentences, create the numpy array with padded\n",
    "    sentences of indexes.\n",
    "\n",
    "    Args:\n",
    "          sentences (list): The list of sentences\n",
    "      tokenizer (keras.preprocessing.text.Tokenizer): The fitter tokenizer\n",
    "\n",
    "    Returns:\n",
    "          The padded sequence of indexes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get initial values for language\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    length = max([len(line.split()) for line in sentences])\n",
    "\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(sentences)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding=\"post\")\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sentence: its my job\n",
      "Transformed: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the variable\n",
    "Y = transform_text_to_sequences(en_sentences, output_tokenizer)\n",
    "\n",
    "# Temporary list\n",
    "ylist = list()\n",
    "for sequence in Y:\n",
    "    # One-hot encode sentence and append to list\n",
    "    ylist.append(to_categorical(sequence, num_classes=en_vocab_size))\n",
    "\n",
    "# Update the variable\n",
    "Y = np.array(ylist).reshape(Y.shape[0], Y.shape[1], en_vocab_size)\n",
    "\n",
    "# Print the raw sentence and its transformed version\n",
    "print(\"Raw sentence: {0}\\nTransformed: {1}\".format(en_sentences[0], Y[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = json.load(open(os.path.join(DATA_PATH, \"models/pt_en_model.json\")))\n",
    "model = tf.keras.models.model_from_json(json.dumps(model))\n",
    "# TODO load weights\n",
    "# model.load_weights(os.path.join(DATA_PATH, \"weights/pt_en_model_weights.h5\"))\n",
    "en_index_to_word = json.load(open(os.path.join(DATA_PATH, \"en_index_to_word.json\")))\n",
    "test = np.array(\n",
    "    [\n",
    "        [\"no way\", \"impossível\"],\n",
    "        [\"were weak\", \"nós estamos fracos\"],\n",
    "        [\"i want soup\", \"eu quero sopa\"],\n",
    "        [\"keep it\", \"fica com ela\"],\n",
    "        [\"she is old\", \"ela é velha\"],\n",
    "        [\"choose one\", \"escolha uma\"],\n",
    "        [\"it was toms\", \"era de tom\"],\n",
    "        [\"we surrender\", \"nós desistimos\"],\n",
    "        [\"ill sue you\", \"eu vou te processar\"],\n",
    "        [\"tom can win\", \"tom pode ganhar\"],\n",
    "        [\"look closely\", \"olhe de perto\"],\n",
    "        [\"im faster\", \"eu sou mais rápida\"],\n",
    "        [\"i feel dizzy\", \"estou tonto\"],\n",
    "        [\"be careful\", \"toma cuidado\"],\n",
    "        [\"dont leave\", \"não saia\"],\n",
    "        [\"im not done\", \"eu ainda não estou pronto\"],\n",
    "        [\"take a rest\", \"descansem\"],\n",
    "        [\"youre sick\", \"você está doente\"],\n",
    "        [\"its a gift\", \"é um presente\"],\n",
    "        [\"tom vomited\", \"o tom vomitou\"],\n",
    "        [\"i apologize\", \"perdão\"],\n",
    "        [\"he ate out\", \"ele comeu fora\"],\n",
    "        [\"be sensible\", \"tende bom senso\"],\n",
    "        [\"i blinked\", \"eu pisquei\"],\n",
    "        [\"be fair\", \"sê justo\"],\n",
    "        [\"i need space\", \"eu preciso de espaço\"],\n",
    "        [\"i will wait\", \"irei esperar\"],\n",
    "        [\"im drunk\", \"estou embriagado\"],\n",
    "        [\"do you mind\", \"você se importa\"],\n",
    "        [\"tom agreed\", \"tom concordou\"],\n",
    "        [\"im drunk\", \"estou bêbada\"],\n",
    "        [\"take mine\", \"pegue o meu\"],\n",
    "        [\"once again\", \"mais uma vez\"],\n",
    "        [\"am i hired\", \"estou contratado\"],\n",
    "        [\"its monday\", \"é segundafeira\"],\n",
    "        [\"thats mine\", \"isso é meu\"],\n",
    "        [\"get up\", \"levantate\"],\n",
    "        [\"i whistled\", \"eu dei um assobio\"],\n",
    "        [\"turn it off\", \"desligueo\"],\n",
    "        [\"whats up\", \"e aí\"],\n",
    "        [\"take it all\", \"leve tudo\"],\n",
    "        [\"tom yelled\", \"tom gritou\"],\n",
    "        [\"come tomorrow\", \"venha amanhã\"],\n",
    "        [\"i ran\", \"eu corri\"],\n",
    "        [\"i am tired\", \"estou cansado\"],\n",
    "        [\"i didnt care\", \"eu não me importo\"],\n",
    "        [\"youre shy\", \"vocês são tímidas\"],\n",
    "        [\"must i hurry\", \"devo me apressar\"],\n",
    "        [\"come to us\", \"venha até nós\"],\n",
    "        [\"do fish sleep\", \"os peixes dormem\"],\n",
    "        [\"stop them\", \"detenhanos\"],\n",
    "        [\"it is unfair\", \"isso é injusto\"],\n",
    "        [\"have a look\", \"dá uma vista de olhos\"],\n",
    "        [\"i won\", \"eu venci\"],\n",
    "        [\"is this cool\", \"isto é legal\"],\n",
    "        [\"im with tom\", \"estou com tom\"],\n",
    "        [\"get serious\", \"fique sério\"],\n",
    "        [\"dont push\", \"não empurrem\"],\n",
    "        [\"ill stay\", \"eu vou ficar\"],\n",
    "        [\"years passed\", \"anos se passaram\"],\n",
    "        [\"i like snow\", \"eu gosto de neve\"],\n",
    "        [\"war is hell\", \"a guerra é um inferno\"],\n",
    "        [\"we miss tom\", \"sentimos saudades do tom\"],\n",
    "        [\"you must go\", \"tu deves partir\"],\n",
    "        [\"i was wrong\", \"eu estava errado\"],\n",
    "        [\"stop lying\", \"pare de mentir\"],\n",
    "        [\"why not\", \"por que não\"],\n",
    "        [\"go inside\", \"entre\"],\n",
    "        [\"i disagree\", \"eu discordo\"],\n",
    "        [\"will he die\", \"ele morrerá\"],\n",
    "        [\"im horrible\", \"eu estou horrível\"],\n",
    "        [\"he was alone\", \"ele estava só\"],\n",
    "        [\"sit tight\", \"peraí\"],\n",
    "        [\"were calm\", \"nós somos tranquilos\"],\n",
    "        [\"this is mine\", \"isto é meu\"],\n",
    "        [\"stay here\", \"fique aqui\"],\n",
    "        [\"look at him\", \"olhe para ele\"],\n",
    "        [\"im punctual\", \"sou pontual\"],\n",
    "        [\"tom listened\", \"tom escutou\"],\n",
    "        [\"i was hired\", \"eu fui contratado\"],\n",
    "        [\"tom hates tv\", \"tom detesta tv\"],\n",
    "        [\"im rich\", \"eu sou rico\"],\n",
    "        [\"tom shaved\", \"tom barbeouse\"],\n",
    "        [\"it may rain\", \"pode chover\"],\n",
    "        [\"im furious\", \"estou furiosa\"],\n",
    "        [\"have a cookie\", \"come um biscoito\"],\n",
    "        [\"whats new\", \"que há de novo\"],\n",
    "        [\"im a doctor\", \"sou médico\"],\n",
    "        [\"dont fight\", \"não lute\"],\n",
    "        [\"he is french\", \"ele é francês\"],\n",
    "        [\"ask an expert\", \"pergunte a um especialista\"],\n",
    "        [\"i said yes\", \"disse sim\"],\n",
    "        [\"you are ugly\", \"você é feio\"],\n",
    "        [\"toms sick\", \"tom está doente\"],\n",
    "        [\"can i hug you\", \"posso te dar um abraço\"],\n",
    "        [\"i want that\", \"eu quero isso\"],\n",
    "        [\"im so bored\", \"estou muito entediado\"],\n",
    "        [\"hes happy\", \"ele é feliz\"],\n",
    "        [\"im wealthy\", \"eu sou rico\"],\n",
    "        [\"dont risk it\", \"não se arrisque\"],\n",
    "        [\"dont get mad\", \"não fique bravo\"],\n",
    "        [\"she is lucky\", \"ela tem sorte\"],\n",
    "        [\"toms crazy\", \"o tom é louco\"],\n",
    "        [\"i handled it\", \"eu lidei com isso\"],\n",
    "        [\"im sleeping\", \"estou dormindo\"],\n",
    "        [\"go ahead\", \"continua\"],\n",
    "        [\"hes stupid\", \"ele é estúpido\"],\n",
    "        [\"unbelievable\", \"inacreditável\"],\n",
    "        [\"hows the dog\", \"como o cachorro está\"],\n",
    "        [\"toms deaf\", \"tom é surdo\"],\n",
    "        [\"come home\", \"venha para casa\"],\n",
    "        [\"do you agree\", \"vocês concordam\"],\n",
    "        [\"tom drowned\", \"tom se afogou\"],\n",
    "        [\"let me leave\", \"deixeme ir embora\"],\n",
    "        [\"tom voted\", \"tom votou\"],\n",
    "        [\"i fear so\", \"eu temo que sim\"],\n",
    "        [\"we all cried\", \"todos nós choramos\"],\n",
    "        [\"ive done it\", \"eu o fiz\"],\n",
    "        [\"is tom out\", \"o tom está fora\"],\n",
    "        [\"dont rush me\", \"não me empurrem\"],\n",
    "        [\"i found it\", \"encontrei\"],\n",
    "        [\"nice shot\", \"belo tiro\"],\n",
    "        [\"hey relax\", \"ei relaxem\"],\n",
    "        [\"weve won\", \"vencemos\"],\n",
    "        [\"im cool\", \"sou legal\"],\n",
    "        [\"watch this\", \"veja isto\"],\n",
    "        [\"im mature\", \"eu sou maturo\"],\n",
    "        [\"ill call\", \"ligarei\"],\n",
    "        [\"tom swore\", \"tom jurou\"],\n",
    "        [\"youre scary\", \"tu és assustador\"],\n",
    "        [\"how are you\", \"como vai você\"],\n",
    "        [\"listen\", \"ouça isso\"],\n",
    "        [\"i went too\", \"eu fui também\"],\n",
    "        [\"tom is on tv\", \"tom está na tv\"],\n",
    "        [\"i can cook\", \"eu posso cozinhar\"],\n",
    "        [\"wait outside\", \"esperem lá fora\"],\n",
    "        [\"well go out\", \"nós vamos sair\"],\n",
    "        [\"did tom help\", \"o tom ajudou\"],\n",
    "        [\"hello girls\", \"olá meninas\"],\n",
    "        [\"stay cool\", \"fique calmo\"],\n",
    "        [\"tom is a pig\", \"tom é um porco\"],\n",
    "        [\"i was fired\", \"eu fui demitido\"],\n",
    "        [\"kiss me\", \"me beije\"],\n",
    "        [\"come over\", \"vem\"],\n",
    "        [\"follow tom\", \"sigam o tom\"],\n",
    "        [\"im involved\", \"estou envolvido\"],\n",
    "        [\"well win\", \"nós venceremos\"],\n",
    "        [\"work slowly\", \"trabalhe devagar\"],\n",
    "        [\"were free\", \"somos livres\"],\n",
    "        [\"please sit\", \"por favor sentemse\"],\n",
    "        [\"he is drunk\", \"ele está bêbado\"],\n",
    "        [\"ill do this\", \"eu vou fazer\"],\n",
    "        [\"you fainted\", \"vocês desmaiaram\"],\n",
    "        [\"keep calm\", \"acalmese\"],\n",
    "        [\"you idiot\", \"seu idiota\"],\n",
    "        [\"he resigned\", \"ele renunciou\"],\n",
    "        [\"how wonderful\", \"maravilhoso\"],\n",
    "        [\"thats mine\", \"é meu\"],\n",
    "        [\"come here\", \"vem cá\"],\n",
    "        [\"i bit my lip\", \"eu mordo o lábio\"],\n",
    "        [\"were dating\", \"estamos namorando\"],\n",
    "        [\"they escaped\", \"escaparam\"],\n",
    "        [\"he cant swim\", \"ele não sabe nadar\"],\n",
    "        [\"run and hide\", \"corra e escondase\"],\n",
    "        [\"i love you\", \"eu te amo\"],\n",
    "        [\"hes wet\", \"ele está molhado\"],\n",
    "        [\"theyre gone\", \"elas se foram\"],\n",
    "        [\"stay with us\", \"fiquem conosco\"],\n",
    "        [\"who is she\", \"quem é ela\"],\n",
    "        [\"they hate me\", \"elas me odeiam\"],\n",
    "        [\"does tom know\", \"tom sabe\"],\n",
    "        [\"we survived\", \"sobrevivemos\"],\n",
    "        [\"stay calm\", \"fique calma\"],\n",
    "        [\"save tom\", \"salve o tom\"],\n",
    "        [\"i love cats\", \"adoro gatos\"],\n",
    "        [\"wood burns\", \"a madeira queima\"],\n",
    "        [\"try it on\", \"experimenteo\"],\n",
    "        [\"do as i asked\", \"faça como eu pedi\"],\n",
    "        [\"he has come\", \"ele veio\"],\n",
    "        [\"aim fire\", \"preparar apontar fogo\"],\n",
    "        [\"i wont lose\", \"eu não vou perder\"],\n",
    "        [\"drive safely\", \"dirija com cuidado\"],\n",
    "        [\"lets review\", \"vamos revisar\"],\n",
    "        [\"be realistic\", \"seja realista\"],\n",
    "        [\"hurry up tom\", \"apressese tom\"],\n",
    "        [\"its free\", \"é de graça\"],\n",
    "        [\"i hate this\", \"eu odeio isto\"],\n",
    "        [\"they came in\", \"eles entraram\"],\n",
    "        [\"come over\", \"venham\"],\n",
    "        [\"i like rice\", \"gosto de arroz\"],\n",
    "        [\"youre kind\", \"você é simpático\"],\n",
    "        [\"youre shy\", \"vocês são tímidos\"],\n",
    "        [\"i am in pain\", \"estou com dor\"],\n",
    "        [\"sign this\", \"assinem isso\"],\n",
    "        [\"toms young\", \"o tom é jovem\"],\n",
    "        [\"come along\", \"vem com a gente\"],\n",
    "        [\"flowers bloom\", \"flores florescem\"],\n",
    "        [\"beware of dog\", \"cuidado com o cão\"],\n",
    "        [\"go ahead\", \"continuem\"],\n",
    "        [\"are you crazy\", \"você é louco\"],\n",
    "        [\"theyre evil\", \"elas são más\"],\n",
    "        [\"this is big\", \"isto é grande\"],\n",
    "        [\"i almost won\", \"eu quase ganhei\"],\n",
    "        [\"come with us\", \"venha com a gente\"],\n",
    "        [\"tom is happy\", \"tom está feliz\"],\n",
    "        [\"i waved\", \"eu acenei\"],\n",
    "        [\"i have a car\", \"eu tenho um carro\"],\n",
    "        [\"my jaw hurts\", \"minha mandíbula dói\"],\n",
    "        [\"tom decided\", \"tom decidiu\"],\n",
    "        [\"watch tom\", \"olha o tom\"],\n",
    "        [\"well obey\", \"nós obedeceremos\"],\n",
    "        [\"go ahead\", \"continue\"],\n",
    "        [\"ill pay you\", \"eu vou te pagar\"],\n",
    "        [\"i will try\", \"eu vou tentar\"],\n",
    "        [\"dont move\", \"não se mexa\"],\n",
    "        [\"unbelievable\", \"incrível\"],\n",
    "        [\"im ready\", \"estou pronta\"],\n",
    "        [\"it stinks\", \"isso fede\"],\n",
    "        [\"im old\", \"eu sou velho\"],\n",
    "        [\"ignore him\", \"ignoreo\"],\n",
    "        [\"that was odd\", \"foi estranho\"],\n",
    "        [\"ill try it\", \"eu vou tentar\"],\n",
    "        [\"is this tom\", \"este é tom\"],\n",
    "        [\"toms angry\", \"tom está com raiva\"],\n",
    "        [\"vote for me\", \"vote em mim\"],\n",
    "        [\"were lazy\", \"nós somos preguiçosas\"],\n",
    "        [\"lets swim\", \"vamos nadar\"],\n",
    "        [\"ill decide\", \"decidirei\"],\n",
    "        [\"its for me\", \"isso é para mim\"],\n",
    "        [\"be tolerant\", \"sê tolerante\"],\n",
    "        [\"come with me\", \"venha comigo\"],\n",
    "        [\"look out\", \"atenção\"],\n",
    "        [\"its hers\", \"isso é dela\"],\n",
    "        [\"is this mine\", \"esse é o meu\"],\n",
    "        [\"hang on tight\", \"segurese firme\"],\n",
    "        [\"tom teaches\", \"tom ensina\"],\n",
    "        [\"i need help\", \"preciso de ajuda\"],\n",
    "        [\"are you upset\", \"você está triste\"],\n",
    "        [\"youve won\", \"você venceu\"],\n",
    "        [\"leave town\", \"deixe a cidade\"],\n",
    "        [\"i dont care\", \"não estou nem aí\"],\n",
    "        [\"im popular\", \"eu sou popular\"],\n",
    "        [\"this is hard\", \"isto é difícil\"],\n",
    "        [\"youre wrong\", \"você está errado\"],\n",
    "        [\"hes smart\", \"ele é inteligente\"],\n",
    "        [\"keep back\", \"afastese\"],\n",
    "        [\"hey dont go\", \"ei não vá\"],\n",
    "        [\"excuse me\", \"com licença\"],\n",
    "        [\"everyone wins\", \"todo mundo sai ganhando\"],\n",
    "        [\"dont be long\", \"não demore\"],\n",
    "        [\"i love her\", \"eu a amo\"],\n",
    "        [\"stay down\", \"fique abaixado\"],\n",
    "        [\"lets play\", \"vamos tocar\"],\n",
    "        [\"bring the key\", \"traga a chave\"],\n",
    "        [\"did tom fall\", \"o tom caiu\"],\n",
    "        [\"youre gross\", \"você é nojento\"],\n",
    "        [\"i did my best\", \"eu fiz o meu melhor\"],\n",
    "        [\"you may stay\", \"pode ficar\"],\n",
    "        [\"taste this\", \"provem isto\"],\n",
    "        [\"fight or die\", \"lute ou morra\"],\n",
    "        [\"ill pay you\", \"irei te pagar\"],\n",
    "        [\"its open\", \"está aberto\"],\n",
    "        [\"i tripped\", \"tropecei\"],\n",
    "        [\"are you brave\", \"você é corajoso\"],\n",
    "        [\"can he do it\", \"ele sabe fazer isso\"],\n",
    "        [\"dont push me\", \"não me empurra\"],\n",
    "        [\"thanks\", \"obrigado\"],\n",
    "        [\"is this true\", \"é verdade\"],\n",
    "        [\"you need me\", \"vocês precisam de mim\"],\n",
    "        [\"tom hates it\", \"o tom odeia isso\"],\n",
    "        [\"i met a girl\", \"eu conheci uma garota\"],\n",
    "        [\"tom is short\", \"o tom é baixo\"],\n",
    "        [\"nobody died\", \"ninguém morreu\"],\n",
    "        [\"let me speak\", \"deixeme falar\"],\n",
    "        [\"come closer\", \"cheguem mais perto\"],\n",
    "        [\"its red\", \"é vermelho\"],\n",
    "        [\"could i do it\", \"eu poderia fazer isso\"],\n",
    "        [\"its done\", \"pronto já está\"],\n",
    "        [\"i need a hug\", \"preciso de um abraço\"],\n",
    "        [\"this is nice\", \"isso é legal\"],\n",
    "        [\"tom is rich\", \"tom é rico\"],\n",
    "        [\"i am busy\", \"estou ocupado\"],\n",
    "        [\"is it a wolf\", \"isso é um lobo\"],\n",
    "        [\"we sat down\", \"nos sentamos\"],\n",
    "        [\"tom grinned\", \"tom sorriu forçadamente\"],\n",
    "        [\"tom escaped\", \"tom escapou\"],\n",
    "        [\"well sing\", \"cantaremos\"],\n",
    "        [\"tomll know\", \"tom vai saber\"],\n",
    "        [\"speak up\", \"falem mais alto\"],\n",
    "        [\"i cant sleep\", \"não consigo dormir\"],\n",
    "        [\"how absurd\", \"que absurdo\"],\n",
    "        [\"im back\", \"voltei\"],\n",
    "        [\"no kidding\", \"sem brincadeira\"],\n",
    "        [\"where is she\", \"cadê ela\"],\n",
    "        [\"ill get up\", \"vou me levantar\"],\n",
    "        [\"im out here\", \"estou aqui fora\"],\n",
    "        [\"tomll lose\", \"tom vai perder\"],\n",
    "        [\"come anytime\", \"venha quando quiser\"],\n",
    "        [\"we got ready\", \"nós nos preparamos\"],\n",
    "        [\"is this ok\", \"está ok\"],\n",
    "        [\"was tom busy\", \"tom estava ocupado\"],\n",
    "        [\"im a farmer\", \"sou um fazendeiro\"],\n",
    "        [\"dont do it\", \"não faça isso\"],\n",
    "        [\"how cute\", \"que fofinho\"],\n",
    "        [\"i will learn\", \"vou aprender\"],\n",
    "        [\"do you drink\", \"vocês bebem\"],\n",
    "        [\"i hope so\", \"assim espero\"],\n",
    "        [\"this is easy\", \"isto é fácil\"],\n",
    "        [\"youre cute\", \"as senhoras são bonitas\"],\n",
    "        [\"itll work\", \"isso vai funcionar\"],\n",
    "        [\"how cute\", \"que bonitinho\"],\n",
    "        [\"tom is calm\", \"tom é calmo\"],\n",
    "        [\"i need money\", \"necessito de dinheiro\"],\n",
    "        [\"sweet dreams\", \"bons sonhos\"],\n",
    "        [\"were sleepy\", \"nós estamos com sono\"],\n",
    "        [\"try hard\", \"se esforça\"],\n",
    "        [\"he was alone\", \"ele estava sozinho\"],\n",
    "        [\"boys run fast\", \"os meninos correm rápido\"],\n",
    "        [\"i need a job\", \"eu preciso de um emprego\"],\n",
    "        [\"can you swim\", \"sabe nadar\"],\n",
    "        [\"i am working\", \"eu estou trabalhando\"],\n",
    "        [\"cool down\", \"acalmemse\"],\n",
    "        [\"pick it up\", \"apanheo\"],\n",
    "        [\"toms ugly\", \"tom é feio\"],\n",
    "        [\"we know why\", \"sabemos por quê\"],\n",
    "        [\"we knew this\", \"nós sabíamos disso\"],\n",
    "        [\"i washed it\", \"eu o lavei\"],\n",
    "        [\"he is happy\", \"ele é feliz\"],\n",
    "        [\"i am like him\", \"eu sou como ele\"],\n",
    "        [\"they won\", \"eles venceram\"],\n",
    "        [\"keep reading\", \"continue lendo\"],\n",
    "        [\"how tragic\", \"que trágico\"],\n",
    "        [\"is tom drunk\", \"o tom está bêbado\"],\n",
    "        [\"lets go eat\", \"vamos comer\"],\n",
    "        [\"im hot\", \"tenho calor\"],\n",
    "        [\"he is alone\", \"ele está sozinho\"],\n",
    "        [\"cain was evil\", \"cain era malvado\"],\n",
    "        [\"how lovely\", \"que amável\"],\n",
    "        [\"ill win\", \"vou vencer\"],\n",
    "        [\"ill get you\", \"vou te pegar\"],\n",
    "        [\"take it easy\", \"acalmese\"],\n",
    "        [\"take tom\", \"leva o tom\"],\n",
    "        [\"toms gone\", \"tom se foi\"],\n",
    "        [\"im the best\", \"eu sou o melhor\"],\n",
    "        [\"tom is mad\", \"tom está insano\"],\n",
    "        [\"were crazy\", \"a gente é maluco\"],\n",
    "        [\"shut up\", \"cala a boca\"],\n",
    "        [\"i will learn\", \"eu aprenderei\"],\n",
    "        [\"i also went\", \"eu também fui\"],\n",
    "        [\"dont ask\", \"não pergunte\"],\n",
    "        [\"contact tom\", \"contacte o tom\"],\n",
    "        [\"contact tom\", \"contactem o tom\"],\n",
    "        [\"youre old\", \"sois velhas\"],\n",
    "        [\"thats true\", \"isso é verdade\"],\n",
    "        [\"have courage\", \"tenha coragem\"],\n",
    "        [\"were hiding\", \"nós estamos nos escondendo\"],\n",
    "        [\"it isnt tom\", \"esse não é o tom\"],\n",
    "        [\"i hate milk\", \"eu odeio leite\"],\n",
    "        [\"go get a beer\", \"vá pegar uma cerveja\"],\n",
    "        [\"start now\", \"comece agora\"],\n",
    "        [\"can we talk\", \"podemos conversar\"],\n",
    "        [\"we need time\", \"precisamos de tempo\"],\n",
    "        [\"toms alive\", \"tom está vivo\"],\n",
    "        [\"start again\", \"começa de novo\"],\n",
    "        [\"im tired\", \"estou cansado\"],\n",
    "        [\"my eyes itch\", \"meus olhos coçam\"],\n",
    "        [\"i have cash\", \"eu tenho dinheiro\"],\n",
    "        [\"you idiot\", \"idiota\"],\n",
    "        [\"are you tired\", \"você está cansada\"],\n",
    "        [\"memorize it\", \"memorize\"],\n",
    "        [\"dont be evil\", \"não seja má\"],\n",
    "        [\"i feel lost\", \"eu me sinto perdido\"],\n",
    "        [\"we agreed\", \"nós concordamos\"],\n",
    "        [\"its too big\", \"é muito grande\"],\n",
    "        [\"im winning\", \"estou vencendo\"],\n",
    "        [\"well try\", \"a gente vai tentar\"],\n",
    "        [\"tom cares\", \"tom se importa\"],\n",
    "        [\"i want a dog\", \"eu quero um cachorro\"],\n",
    "        [\"im broke\", \"estou quebrado\"],\n",
    "        [\"i like that\", \"curto isso\"],\n",
    "        [\"well win\", \"venceremos\"],\n",
    "        [\"are you ok\", \"vocês estão bem\"],\n",
    "        [\"nobody came\", \"não veio ninguém\"],\n",
    "        [\"ignore them\", \"ignoreas\"],\n",
    "        [\"i am single\", \"estou solteiro\"],\n",
    "        [\"get some rest\", \"descanse um pouco\"],\n",
    "        [\"i was bored\", \"eu estava entediada\"],\n",
    "        [\"have fun\", \"divertete\"],\n",
    "        [\"i just ate\", \"acabei de comer\"],\n",
    "        [\"im sorry\", \"me desculpe\"],\n",
    "        [\"im hungry\", \"quero comer\"],\n",
    "        [\"keep it\", \"fiquem com ela\"],\n",
    "        [\"ive no idea\", \"não tenho ideia\"],\n",
    "        [\"i must study\", \"eu preciso estudar\"],\n",
    "        [\"i need more\", \"preciso de mais\"],\n",
    "        [\"im fighting\", \"eu estou lutando\"],\n",
    "        [\"tom refused\", \"tom recusou\"],\n",
    "        [\"im broke\", \"eu estou duro\"],\n",
    "        [\"tom is weak\", \"tom é fraco\"],\n",
    "        [\"i slipped\", \"eu escorreguei\"],\n",
    "        [\"am i early\", \"cheguei cedo\"],\n",
    "        [\"im busy\", \"estou ocupado\"],\n",
    "        [\"i was sick\", \"eu estava doente\"],\n",
    "        [\"they fell\", \"sentiram\"],\n",
    "        [\"whats wrong\", \"o que está errado\"],\n",
    "        [\"tom felt sad\", \"tom se sentiu triste\"],\n",
    "        [\"is it for me\", \"é para mim\"],\n",
    "        [\"youre wise\", \"você é sábio\"],\n",
    "        [\"i saw that\", \"eu vi\"],\n",
    "        [\"get real\", \"acorda\"],\n",
    "        [\"keep digging\", \"continuem cavando\"],\n",
    "        [\"youre old\", \"você é velha\"],\n",
    "        [\"hes too busy\", \"ele está muito ocupado\"],\n",
    "        [\"we remember\", \"nós lembramos\"],\n",
    "        [\"get out\", \"saia\"],\n",
    "        [\"im fine now\", \"eu estou bem agora\"],\n",
    "        [\"go on\", \"vá\"],\n",
    "        [\"youre brave\", \"você é valente\"],\n",
    "        [\"i was hiding\", \"eu estava me escondendo\"],\n",
    "        [\"im a vegan\", \"eu sou vegana\"],\n",
    "        [\"it helps\", \"isso ajuda\"],\n",
    "        [\"tom failed\", \"tom falhou\"],\n",
    "        [\"im through\", \"acabei\"],\n",
    "        [\"youre shy\", \"você está tímida\"],\n",
    "        [\"help me tom\", \"ajudeme tom\"],\n",
    "        [\"well wait\", \"nós vamos esperar\"],\n",
    "        [\"im special\", \"eu sou especial\"],\n",
    "        [\"tom tripped\", \"tom disparou\"],\n",
    "        [\"well scream\", \"nós vamos gritar\"],\n",
    "        [\"tom told us\", \"tom nos disse\"],\n",
    "        [\"i laughed\", \"ri\"],\n",
    "        [\"attack\", \"atacar\"],\n",
    "        [\"help me\", \"ajudeme\"],\n",
    "        [\"get a move on\", \"mexase\"],\n",
    "        [\"it happens\", \"isso acontece\"],\n",
    "        [\"wake up\", \"acordem\"],\n",
    "        [\"im a farmer\", \"eu sou um agricultor\"],\n",
    "        [\"wake up tom\", \"acorda tom\"],\n",
    "        [\"i need air\", \"eu preciso de ar\"],\n",
    "        [\"get down\", \"deitemse\"],\n",
    "        [\"are they gone\", \"elas foram embora\"],\n",
    "        [\"theyre back\", \"eles voltaram\"],\n",
    "        [\"am i right\", \"eu estou certo\"],\n",
    "        [\"goodbye\", \"tchau\"],\n",
    "        [\"tom will pay\", \"tom vai pagar\"],\n",
    "        [\"i dont snore\", \"eu não ronco\"],\n",
    "        [\"he can read\", \"ele sabe ler\"],\n",
    "        [\"tom got sick\", \"tom ficou doente\"],\n",
    "        [\"do it for tom\", \"façam isso pelo tom\"],\n",
    "        [\"im like tom\", \"eu sou como o tom\"],\n",
    "        [\"someone came\", \"alguém veio\"],\n",
    "        [\"well attack\", \"atacaremos\"],\n",
    "        [\"im managing\", \"estou conseguindo\"],\n",
    "        [\"im lying\", \"estou mentindo\"],\n",
    "        [\"im dieting\", \"estou de dieta\"],\n",
    "        [\"what a pain\", \"que dor\"],\n",
    "        [\"bring backup\", \"tragam reforço\"],\n",
    "        [\"i like that\", \"eu gosto disto\"],\n",
    "        [\"how pathetic\", \"que patético\"],\n",
    "        [\"do it again\", \"faz outra vez\"],\n",
    "        [\"i liked tom\", \"gostei de tom\"],\n",
    "        [\"im thin\", \"eu sou magro\"],\n",
    "        [\"help me out\", \"ajudeme a sair\"],\n",
    "        [\"sing along\", \"cante junto\"],\n",
    "        [\"hes not sick\", \"ele não está doente\"],\n",
    "        [\"tom won\", \"tom ganhou\"],\n",
    "        [\"did tom reply\", \"o tom respondeu\"],\n",
    "        [\"i like girls\", \"gosto de meninas\"],\n",
    "        [\"ill explain\", \"explicarei\"],\n",
    "        [\"was she seen\", \"ela foi vista\"],\n",
    "        [\"thats good\", \"isso é bom\"],\n",
    "        [\"dont look up\", \"não olhe para cima\"],\n",
    "        [\"he shot at me\", \"ele atirou em mim\"],\n",
    "        [\"tom likes me\", \"o tom gosta de mim\"],\n",
    "        [\"were late\", \"estamos atrasados\"],\n",
    "        [\"give him time\", \"dêlhe tempo\"],\n",
    "        [\"have another\", \"tome outro\"],\n",
    "        [\"are you home\", \"você está em casa\"],\n",
    "        [\"i dont dream\", \"eu não sonho\"],\n",
    "        [\"tom waved\", \"tom acenou\"],\n",
    "        [\"im hungry\", \"estou faminto\"],\n",
    "        [\"i am a boy\", \"eu sou um menino\"],\n",
    "        [\"i cut myself\", \"eu me cortei\"],\n",
    "        [\"listen\", \"escute\"],\n",
    "        [\"i am sure\", \"tenho certeza\"],\n",
    "        [\"its locked\", \"está trancado\"],\n",
    "        [\"look at that\", \"olhe para aquilo\"],\n",
    "        [\"i believe tom\", \"eu acredito no tom\"],\n",
    "        [\"he mocked me\", \"ele zombou de mim\"],\n",
    "        [\"who escaped\", \"quem escapou\"],\n",
    "        [\"i drank milk\", \"eu bebi leite\"],\n",
    "        [\"did i win\", \"eu ganhei\"],\n",
    "        [\"be still\", \"acalmese\"],\n",
    "        [\"i keep a dog\", \"eu tenho um cachorro\"],\n",
    "        [\"stop smoking\", \"pare de fumar\"],\n",
    "        [\"call me later\", \"me liga depois\"],\n",
    "        [\"look alive\", \"se apresse\"],\n",
    "        [\"id buy that\", \"eu compraria aquele\"],\n",
    "        [\"i have won\", \"eu venci\"],\n",
    "        [\"youre old\", \"você está velho\"],\n",
    "    ],\n",
    "    dtype=\"<U195\",\n",
    ")\n",
    "X_test = np.array(\n",
    "    json.load(open(os.path.join(DATA_PATH, \"nmt/x_test.json\"))), dtype=\"int32\"\n",
    ")\n",
    "\n",
    "\n",
    "def predict_one(model, sentence, index_to_word):\n",
    "    \"\"\"Translate one sentence\n",
    "\n",
    "    Uses the pre-trained model to translate one Portuguese sentence into English.\n",
    "\n",
    "    Args:\n",
    "        model (keras.models.Sequential): The pre-trained model.\n",
    "        sentence (string): The Portuguese sentence to translate.\n",
    "        index_to_word (dict): Dictionary containing indexes as keys and words (English) as values.\n",
    "\n",
    "    Returns:\n",
    "        The translated sentence.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = sentence.reshape((1, sentence.shape[0]))\n",
    "    prediction = model.predict(sentence, verbose=0)[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = index_to_word.get(i, None)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return \" \".join(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src=[impossível], target=[no way], predicted=[]\n",
      "src=[nós estamos fracos], target=[were weak], predicted=[]\n",
      "src=[eu quero sopa], target=[i want soup], predicted=[]\n",
      "src=[fica com ela], target=[keep it], predicted=[]\n",
      "src=[ela é velha], target=[she is old], predicted=[]\n",
      "src=[escolha uma], target=[choose one], predicted=[]\n",
      "src=[era de tom], target=[it was toms], predicted=[]\n",
      "src=[nós desistimos], target=[we surrender], predicted=[]\n",
      "src=[eu vou te processar], target=[ill sue you], predicted=[]\n",
      "src=[tom pode ganhar], target=[tom can win], predicted=[]\n"
     ]
    }
   ],
   "source": [
    "# Function to predict many phrases\n",
    "def predict_many(model, sentences, index_to_word, raw_dataset):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Translate the Portuguese sentence\n",
    "        translation = predict_one(model, sentence, index_to_word)\n",
    "\n",
    "        # Get the raw Portuguese and English sentences\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "\n",
    "        # Print the correct Portuguese and English sentences and the predicted\n",
    "        print(\n",
    "            \"src=[%s], target=[%s], predicted=[%s]\" % (raw_src, raw_target, translation)\n",
    "        )\n",
    "\n",
    "\n",
    "predict_many(model, X_test[:10], en_index_to_word, test)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19da0f01cc827b63f7931b6fe8533bf475b989a32e5c993fa3c78c19e4be6d69"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tfenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
