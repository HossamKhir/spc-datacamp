{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "\n",
    "DATA_PATH = \"../data/raw\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, \"sentences.txt\")) as sentences:\n",
    "    sentences = sentences.readlines()\n",
    "\n",
    "model = load_model(os.path.join(DATA_PATH, \"sent_analysis.h5\"))\n",
    "X_test = np.load(os.path.join(DATA_PATH, \"sent_analysis/x_test.npy\"))\n",
    "y_test = np.load(os.path.join(DATA_PATH, \"sent_analysis/y_test.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2]\n",
      " [   5]\n",
      " [  68]\n",
      " [   2]\n",
      " [1315]\n",
      " [   9]\n",
      " [  12]\n",
      " [  32]\n",
      " [  43]\n",
      " [  44]\n",
      " [ 397]\n",
      " [2128]\n",
      " [  13]\n",
      " [ 963]\n",
      " [4637]\n",
      " [  39]\n",
      " [  68]\n",
      " [   2]\n",
      " [ 332]\n",
      " [   2]\n",
      " [  39]\n",
      " [  68]\n",
      " [   2]\n",
      " [   2]\n",
      " [  14]\n",
      " [ 418]\n",
      " [   7]\n",
      " [ 595]\n",
      " [   2]\n",
      " [   4]\n",
      " [ 130]\n",
      " [   7]\n",
      " [   6]\n",
      " [3592]\n",
      " [   7]\n",
      " [  52]\n",
      " [   5]\n",
      " [  87]\n",
      " [ 102]\n",
      " [   2]\n",
      " [  93]\n",
      " [  11]\n",
      " [   4]\n",
      " [ 402]\n",
      " [   5]\n",
      " [1696]\n",
      " [4773]\n",
      " [ 141]\n",
      " [  17]\n",
      " [   2]\n",
      " [ 251]\n",
      " [1605]\n",
      " [ 653]\n",
      " [1168]\n",
      " [ 912]\n",
      " [3295]\n",
      " [   2]\n",
      " [  51]\n",
      " [  44]\n",
      " [2046]\n",
      " [   5]\n",
      " [1659]\n",
      " [2137]\n",
      " [1033]\n",
      " [2002]\n",
      " [  69]\n",
      " [   4]\n",
      " [   2]\n",
      " [1742]\n",
      " [   7]\n",
      " [ 319]\n",
      " [  90]\n",
      " [  11]\n",
      " [1244]\n",
      " [   2]\n",
      " [ 141]\n",
      " [  17]\n",
      " [3420]\n",
      " [ 416]\n",
      " [  11]\n",
      " [3853]\n",
      " [  25]\n",
      " [  43]\n",
      " [ 191]\n",
      " [  79]\n",
      " [ 245]\n",
      " [  39]\n",
      " [ 134]\n",
      " [3282]\n",
      " [1020]\n",
      " [   5]\n",
      " [1912]\n",
      " [   2]\n",
      " [1661]\n",
      " [ 148]\n",
      " [ 107]\n",
      " [  10]\n",
      " [  10]\n",
      " [  31]\n",
      " [ 232]\n",
      " [2209]\n",
      " [ 163]\n",
      " [  17]\n",
      " [2304]\n",
      " [ 150]\n",
      " [ 198]\n",
      " [   4]\n",
      " [ 243]\n",
      " [   7]\n",
      " [ 311]\n",
      " [  14]\n",
      " [  20]\n",
      " [  16]\n",
      " [1383]\n",
      " [  18]\n",
      " [   2]\n",
      " [   2]\n",
      " [   5]\n",
      " [1472]\n",
      " [   2]]\n",
      "                                            sentence    y_pred    y_true\n",
      "0  the it of yet br stress and must in at town wh...  positive  negative\n",
      "1  the what have just be ever have 2 at is over d...  negative  positive\n",
      "2  the was me of and in character and performance...  negative  positive\n",
      "3  the as on mean unlike and movie pictures is pa...  negative  negative\n",
      "4  the genuine was capture now and and and new to...  negative  negative\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first sentence on `X_test`\n",
    "print(X_test[0])\n",
    "\n",
    "# Get the predicion for all the sentences\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Transform the predition into positive (> 0.5) or negative (<= 0.5)\n",
    "pred_sentiment = [\"positive\" if x > 0.5 else \"negative\" for x in pred]\n",
    "\n",
    "# Create a data frame with sentences, predictions and true values\n",
    "result = pd.DataFrame(\n",
    "    {\"sentence\": sentences, \"y_pred\": pred_sentiment, \"y_true\": y_test}\n",
    ")\n",
    "\n",
    "# Print the first lines of the data frame\n",
    "print(result.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheldon_quotes = [\n",
    "    \"You're afraid of insects and women, Ladybugs must render you catatonic.\",\n",
    "    \"Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.\",\n",
    "    \"For example, I cry because others are stupid, and that makes me sad.\",\n",
    "    \"I'm not insane, my mother had me tested.\",\n",
    "    \"Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.\",\n",
    "    \"Amy's birthday present will be my genitals.\",\n",
    "    \"(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!\",\n",
    "    \"Thankfully all the things my girlfriend used to do can be taken care of with my right hand.\",\n",
    "    \"I would have been here sooner but the bus kept stopping for other people to get on it.\",\n",
    "    \"Oh gravity, thou art a heartless bitch.\",\n",
    "    \"I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.\",\n",
    "    \"Well, today we tried masturbating for money.\",\n",
    "    \"I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.\",\n",
    "    \"Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.\",\n",
    "    \"What computer do you have? And please don't say a white one.\",\n",
    "    \"She calls me moon-pie because I'm nummy-nummy and she could just eat me up.\",\n",
    "    \"Ah, memory impairment; the free prize at the bottom of every vodka bottle.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '(3', 1: 'Ah,', 2: \"Amy's\", 3: 'And', 4: 'Explorer', 5: 'Firefox.', 6: 'For', 7: 'Galileo,', 8: 'Goblin', 9: 'Green', 10: 'Hubble', 11: 'I', 12: \"I'm\", 13: 'Internet', 14: 'Ladybugs', 15: 'Oh', 16: 'Paul', 17: 'Penny', 18: 'Penny!', 19: 'Pope', 20: 'Scissors', 21: 'She', 22: 'Spider-Man,', 23: 'Spock', 24: 'Spock,', 25: 'Thankfully', 26: 'The', 27: 'Two', 28: 'V', 29: 'Well,', 30: 'What', 31: 'Wheaton!', 32: 'Wil', 33: \"You're\", 34: 'a', 35: 'afraid', 36: 'all', 37: 'always', 38: 'am', 39: 'and', 40: 'appeals', 41: 'are', 42: 'art', 43: 'as', 44: 'at', 45: 'aware', 46: 'based', 47: 'be', 48: 'became', 49: 'because', 50: 'been', 51: 'birthday', 52: 'bitch.', 53: 'black', 54: 'blood', 55: 'bottle.', 56: 'bottom', 57: 'brain', 58: 'breaker.', 59: 'bus', 60: 'but', 61: 'calls', 62: 'can', 63: 'care', 64: 'catatonic.', 65: 'center', 66: 'chance', 67: 'circuit', 68: 'computer', 69: 'could', 70: 'covers', 71: 'crushes', 72: 'cry', 73: 'cuts', 74: 'days', 75: 'decapitates', 76: 'deity.', 77: 'discovering', 78: 'disproves', 79: 'do', 80: 'does', 81: \"don't\", 82: 'eat', 83: 'eats', 84: 'every', 85: 'example,', 86: 'flashlight', 87: 'for', 88: 'free', 89: 'genitals,', 90: 'genitals.', 91: 'get', 92: 'ghost', 93: 'girlfriend', 94: 'gravity,', 95: 'had', 96: 'hand.', 97: 'has,', 98: 'have', 99: 'have?', 100: 'having', 101: 'heartless', 102: 'here', 103: 'hole', 104: 'humans', 105: 'if', 106: 'impairment;', 107: 'in', 108: 'insane,', 109: 'insects', 110: 'involves', 111: 'is', 112: \"isn't\", 113: 'it', 114: 'it.', 115: 'just', 116: 'kept', 117: 'knocks)', 118: 'later,', 119: 'little', 120: 'living', 121: 'lizard', 122: 'lizard,', 123: 'loud', 124: 'makes', 125: 'man', 126: 'masturbating', 127: 'me', 128: 'memory', 129: 'messy,', 130: 'money.', 131: 'moon-pie', 132: 'mother', 133: 'moved', 134: 'much', 135: 'must', 136: 'my', 137: 'next', 138: 'not', 139: 'nummy-nummy', 140: 'of', 141: 'on', 142: 'one.', 143: 'other', 144: 'others', 145: 'paper', 146: 'paper,', 147: 'people', 148: 'please', 149: 'poisons', 150: 'present', 151: 'prize', 152: 'relationship', 153: 'render', 154: 'reproduce', 155: 'right', 156: 'rock', 157: 'rock,', 158: 'rushed', 159: 'sad.', 160: 'say', 161: 'scissors', 162: 'scissors,', 163: 'scissors.', 164: 'searching', 165: 'sexual', 166: 'she', 167: 'smashes', 168: 'so', 169: 'sooner', 170: 'stopping', 171: 'stupid,', 172: 'taken', 173: 'telescope', 174: 'tested.', 175: 'that', 176: 'the', 177: 'things', 178: 'think', 179: 'thou', 180: 'three', 181: 'to', 182: 'today', 183: 'town.', 184: 'tried', 185: 'unnecessary', 186: 'unsanitary', 187: 'up.', 188: 'used', 189: 'usually', 190: 'vaporizes', 191: 'vodka', 192: 'way', 193: 'we', 194: 'well,', 195: 'which', 196: 'white', 197: 'will', 198: 'with', 199: 'women,', 200: 'would', 201: 'years,', 202: 'you', 203: 'your'}\n",
      "{'(3': 0, 'Ah,': 1, \"Amy's\": 2, 'And': 3, 'Explorer': 4, 'Firefox.': 5, 'For': 6, 'Galileo,': 7, 'Goblin': 8, 'Green': 9, 'Hubble': 10, 'I': 11, \"I'm\": 12, 'Internet': 13, 'Ladybugs': 14, 'Oh': 15, 'Paul': 16, 'Penny': 17, 'Penny!': 18, 'Pope': 19, 'Scissors': 20, 'She': 21, 'Spider-Man,': 22, 'Spock': 23, 'Spock,': 24, 'Thankfully': 25, 'The': 26, 'Two': 27, 'V': 28, 'Well,': 29, 'What': 30, 'Wheaton!': 31, 'Wil': 32, \"You're\": 33, 'a': 34, 'afraid': 35, 'all': 36, 'always': 37, 'am': 38, 'and': 39, 'appeals': 40, 'are': 41, 'art': 42, 'as': 43, 'at': 44, 'aware': 45, 'based': 46, 'be': 47, 'became': 48, 'because': 49, 'been': 50, 'birthday': 51, 'bitch.': 52, 'black': 53, 'blood': 54, 'bottle.': 55, 'bottom': 56, 'brain': 57, 'breaker.': 58, 'bus': 59, 'but': 60, 'calls': 61, 'can': 62, 'care': 63, 'catatonic.': 64, 'center': 65, 'chance': 66, 'circuit': 67, 'computer': 68, 'could': 69, 'covers': 70, 'crushes': 71, 'cry': 72, 'cuts': 73, 'days': 74, 'decapitates': 75, 'deity.': 76, 'discovering': 77, 'disproves': 78, 'do': 79, 'does': 80, \"don't\": 81, 'eat': 82, 'eats': 83, 'every': 84, 'example,': 85, 'flashlight': 86, 'for': 87, 'free': 88, 'genitals,': 89, 'genitals.': 90, 'get': 91, 'ghost': 92, 'girlfriend': 93, 'gravity,': 94, 'had': 95, 'hand.': 96, 'has,': 97, 'have': 98, 'have?': 99, 'having': 100, 'heartless': 101, 'here': 102, 'hole': 103, 'humans': 104, 'if': 105, 'impairment;': 106, 'in': 107, 'insane,': 108, 'insects': 109, 'involves': 110, 'is': 111, \"isn't\": 112, 'it': 113, 'it.': 114, 'just': 115, 'kept': 116, 'knocks)': 117, 'later,': 118, 'little': 119, 'living': 120, 'lizard': 121, 'lizard,': 122, 'loud': 123, 'makes': 124, 'man': 125, 'masturbating': 126, 'me': 127, 'memory': 128, 'messy,': 129, 'money.': 130, 'moon-pie': 131, 'mother': 132, 'moved': 133, 'much': 134, 'must': 135, 'my': 136, 'next': 137, 'not': 138, 'nummy-nummy': 139, 'of': 140, 'on': 141, 'one.': 142, 'other': 143, 'others': 144, 'paper': 145, 'paper,': 146, 'people': 147, 'please': 148, 'poisons': 149, 'present': 150, 'prize': 151, 'relationship': 152, 'render': 153, 'reproduce': 154, 'right': 155, 'rock': 156, 'rock,': 157, 'rushed': 158, 'sad.': 159, 'say': 160, 'scissors': 161, 'scissors,': 162, 'scissors.': 163, 'searching': 164, 'sexual': 165, 'she': 166, 'smashes': 167, 'so': 168, 'sooner': 169, 'stopping': 170, 'stupid,': 171, 'taken': 172, 'telescope': 173, 'tested.': 174, 'that': 175, 'the': 176, 'things': 177, 'think': 178, 'thou': 179, 'three': 180, 'to': 181, 'today': 182, 'town.': 183, 'tried': 184, 'unnecessary': 185, 'unsanitary': 186, 'up.': 187, 'used': 188, 'usually': 189, 'vaporizes': 190, 'vodka': 191, 'way': 192, 'we': 193, 'well,': 194, 'which': 195, 'white': 196, 'will': 197, 'with': 198, 'women,': 199, 'would': 200, 'years,': 201, 'you': 202, 'your': 203}\n"
     ]
    }
   ],
   "source": [
    "# Transform the list of sentences into a list of words\n",
    "all_words = \" \".join(sheldon_quotes).split(\" \")\n",
    "\n",
    "# Get number of unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Dictionary of indexes as keys and words as values\n",
    "index_to_word = {i: wd for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(index_to_word)\n",
    "\n",
    "# Dictionary of words as keys and indexes as values\n",
    "word_to_index = {wd: i for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(word_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples(sentences, next_chars, n=10):\n",
    "    \"\"\"Function to print examples of (data,label)\n",
    "\n",
    "    This function loops over the sentences and prints the pair of data and label,\n",
    "    corresponding to the sentence and next char.\n",
    "    This way, the student can check how the data was transformed.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): the prepared data\n",
    "        next_chars (string): the label containing the next char of the sentences\n",
    "        n (int): the number of examples to print\n",
    "\n",
    "    Returns:\n",
    "        nothing\n",
    "\n",
    "    \"\"\"\n",
    "    result = \"Sentence\\tNext char\\n\"\n",
    "    n_i = 1\n",
    "    for sent, char in zip(sentences, next_chars):\n",
    "        if n_i >= n:\n",
    "            break\n",
    "        result = result + sent + \"\\t\" + char + \"\\n\"\n",
    "        n_i += 1\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheldon_quotes = \"\".join(sheldon_quotes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence\tNext char\n",
      "You're afr\ta\n",
      "u're afrai\td\n",
      "re afraid \to\n",
      " afraid of\t \n",
      "fraid of i\tn\n",
      "aid of ins\te\n",
      "d of insec\tt\n",
      "of insects\t \n",
      " insects a\tn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create lists to keep the sentences and the next character\n",
    "sentences = []  # ~ Training data\n",
    "next_chars = []  # ~ Training labels\n",
    "\n",
    "# Define hyperparameters\n",
    "step = 2  # ~ Step to take when reading the texts in characters\n",
    "chars_window = 10  # ~ Number of characters to use to predict the next one\n",
    "\n",
    "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
    "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
    "    sentences.append(sheldon_quotes[i : i + chars_window])\n",
    "    next_chars.append(sheldon_quotes[i + chars_window])\n",
    "\n",
    "# Print 10 pairs\n",
    "print_examples(sentences, next_chars, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = [\n",
    "    \"A man either lives life as it happens to him meets it head-on and licks it or he turns his back on it and starts to wither away\",\n",
    "    \"To the brave crew and passengers of the Kobayshi Maru sucks to be you\",\n",
    "    \"Beware of more powerful weapons They often inflict as much damage to your soul as they do to you enemies\",\n",
    "    \"They are merely scars not mortal wounds and you must use them to propel you forward\",\n",
    "    \"You cannot explain away a wantonly immoral act because you think that it is connected to some higher purpose\",\n",
    "]\n",
    "\n",
    "index_to_word = pickle.load(open(os.path.join(DATA_PATH, \"index_to_word.pkl\"), \"rb\"))\n",
    "word_to_index = pickle.load(open(os.path.join(DATA_PATH, \"word_to_index.pkl\"), \"rb\"))\n",
    "vocabulary = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, \"vocabulary.csv\"), header=None\n",
    ").values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[276, 15070, 10160, 14750, 14590, 5715, 13813, 12418, 22564, 12797, 15443, 13813, 0, 5368, 14578, 13813, 16947, 12507, 23031, 12859, 5975, 16795, 13813, 5368, 21189, 22564, 0, 5910]\n",
      "A man either lives life as it happens to him meets it <UKN/> and licks it or he turns his back on it and starts to <UKN/> away\n"
     ]
    }
   ],
   "source": [
    "# Loop through the sentences and get indexes\n",
    "new_text_split = []\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        index = word_to_index.get(wd, 0)\n",
    "        sent_split.append(index)\n",
    "    new_text_split.append(sent_split)\n",
    "\n",
    "# Print the first sentence's indexes\n",
    "print(new_text_split[0])\n",
    "\n",
    "# Print the sentence converted using the dictionary\n",
    "print(' '.join([index_to_word[index] for index in new_text_split[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs in Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, LSTM, Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " LSTM (LSTM)                 (None, 128)               71168     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,297\n",
      "Trainable params: 71,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the class\n",
    "model = Sequential(name=\"sequential_model\")\n",
    "\n",
    "# One LSTM layer (defining the input shape because it is the\n",
    "# initial layer)\n",
    "model.add(LSTM(128, input_shape=(None, 10), name=\"LSTM\"))\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
    "\n",
    "# The summary shows the layers and the number of parameters\n",
    "# that will be trained\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"modelclass_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, None, 10)]        0         \n",
      "                                                                 \n",
      " LSTM (LSTM)                 (None, 128)               71168     \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,297\n",
      "Trainable params: 71,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the input layer\n",
    "main_input = Input(shape=(None, 10), name=\"input\")\n",
    "\n",
    "# One LSTM layer (input shape is already defined)\n",
    "lstm_layer = LSTM(128, name=\"LSTM\")(main_input)\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "main_output = Dense(1, activation=\"sigmoid\", name=\"output\")(lstm_layer)\n",
    "\n",
    "# Instantiate the class at the end\n",
    "model = Model(inputs=main_input, outputs=main_output, name=\"modelclass_model\")\n",
    "\n",
    "# Same amount of parameters to train as before (71,297)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.load(os.path.join(DATA_PATH, \"texts.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the sample texts: (54, 78)\n",
      "Now the texts have fixed length: 60. Let's see the first one: \n",
      "[ 0  0  0  0  0  0 24  4  1 25 13 26  5  1 14  3 27  6 28  2  7 29 30 13\n",
      " 15  2  8 16 17  5 18  6  4  9 31  2  8 32  4  9 15 33  9 34 35 14 36 37\n",
      "  2 38 39 40  2  8 16 41 42  5 18  6]\n"
     ]
    }
   ],
   "source": [
    "# Import relevant classes/functions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Build the dictionary of indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Change texts into sequence of indexes\n",
    "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
    "print(\n",
    "    \"Number of words in the sample texts: ({0}, {1})\".format(\n",
    "        len(texts_numeric[0]), len(texts_numeric[1])\n",
    "    )\n",
    ")\n",
    "\n",
    "# Pad the sequences\n",
    "texts_pad = pad_sequences(texts_numeric, 60)\n",
    "print(\n",
    "    \"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(\n",
    "        texts_pad[0]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "\n",
    "x_test = np.load(os.path.join(DATA_PATH, \"rnn/x_test.npy\"))\n",
    "y_test = np.load(os.path.join(DATA_PATH, \"rnn/y_test.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6991181373596191 \n",
      "Accuracy: 0.4950000047683716\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=128, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights(os.path.join(DATA_PATH, \"model_weights.h5\"))\n",
    "\n",
    "# Method '.evaluate()' shows the loss and accuracy\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Loss: {0} \\nAccuracy: {1}\".format(loss, acc))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19da0f01cc827b63f7931b6fe8533bf475b989a32e5c993fa3c78c19e4be6d69"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('tfenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
